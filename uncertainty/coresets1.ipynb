{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tqdm import tqdm_notebook\n",
    "from tqdm.notebook import tqdm, tnrange\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "import numpy as np\n",
    "import glob\n",
    "import sys, os\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "from utils import gdal_processing as gp\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from skimage.measure import block_reduce\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_reader import interpPatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path_figs = '/scratch2/Dropbox/Dropbox/Apps/Overleaf/activelearning_remotesensing/figures/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Sumatra and Peninsular Malaysia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_gt = '/scratch/andresro/leon_work/barry_palm/data/labels/manual_annotations'\n",
    "path_gt = '/home/pf/pfstaff/projects/andresro/barry_palm/data/labels/manual_annotations'\n",
    "\n",
    "tiles_gt_all = glob.glob(path_gt+'/T46*/palm*')\n",
    "tiles_gt_all += glob.glob(path_gt+'/T47*/palm*')\n",
    "tiles_gt_all += glob.glob(path_gt+'/T48*/palm*')\n",
    "\n",
    "tiles_gt_all = sorted(tiles_gt_all)\n",
    "print(len(tiles_gt_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rois_gt_all = np.array([gp.get_positive_area_folder(tile_) for tile_ in tiles_gt_all])\n",
    "\n",
    "def middle_roi(roi_list):\n",
    "    return np.array([roi_list[:,0]+roi_list[:,2],roi_list[:,1]+roi_list[:,3]]).T/2\n",
    "\n",
    "rois_middle = middle_roi(rois_gt_all)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = geopandas.GeoDataFrame(\n",
    "    geometry=geopandas.points_from_xy(rois_middle[:,0], rois_middle[:,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_ = '/scratch/andresro/leon_work/barry_palm/data/labels/manual_annotations'\n",
    "# index_  = [tiles_available.index(path_+x) for x in out_dict['10a']]\n",
    "# index_bool = [x in index_ for x in list(gdf.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gdf['Sample'] = 'No'\n",
    "#gdf['Sample'].loc[nonuniform_samples > 0] = 'Non Uniform'\n",
    "#gdf['Sample'].loc[uniform_samples > 0] = 'Random'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))\n",
    "countries = geopandas.read_file(\"/home/pf/pfstaff/projects/andresro/barry_palm/data/labels/countries/3countries.shp\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "world.plot(ax=ax,\n",
    "    color='lightgray', edgecolor='white')\n",
    "\n",
    "countries.plot(ax=ax,\n",
    "    color='lightgray', edgecolor='white')\n",
    "\n",
    "\n",
    "gdf.plot(ax=ax, marker='.')\n",
    "\n",
    "minx, miny, maxx, maxy = gdf.total_bounds\n",
    "margin_x = 2\n",
    "margin_y = 2\n",
    "\n",
    "ax.set_xlim(minx-margin_x, maxx+margin_x)\n",
    "ax.set_ylim(miny-margin_y, maxy+margin_y)\n",
    "plt.title('Available samples')\n",
    "# plt.savefig('active_learning_samples_10a.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing Base train and val samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(tiles_gt_all)\n",
    "np.random.seed(1)\n",
    "index = np.arange(N)\n",
    "np.random.shuffle(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_train = index[0:10]\n",
    "index_val = index[10:20]\n",
    "index_avail = index[20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_train = np.array(tiles_gt_all)[index_train]\n",
    "tiles_val = np.array(tiles_gt_all)[index_val]\n",
    "tiles_avail = np.array(tiles_gt_all)[index_avail]\n",
    "\n",
    "assert len(set(tiles_train).intersection(set(tiles_val))) == 0\n",
    "\n",
    "assert len(set(tiles_val).intersection(set(tiles_avail))) == 0\n",
    "assert len(set(tiles_train).intersection(set(tiles_avail))) == 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))\n",
    "countries = geopandas.read_file(\"/home/pf/pfstaff/projects/andresro/barry_palm/data/labels/countries/3countries.shp\")\n",
    "\n",
    "\n",
    "#fig, ax = plt.subplots(figsize=(5,5))\n",
    "plt.figure(figsize=(6,6))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "\n",
    "world.plot(ax=ax,\n",
    "    color='lightgray', edgecolor='white')\n",
    "\n",
    "countries.plot(ax=ax,\n",
    "     color='lightgray', edgecolor='white')\n",
    "\n",
    "\n",
    "\n",
    "gdf.plot(ax=ax,legend=True, marker='.',\n",
    "          edgecolor='black',\n",
    "          facecolor='none',\n",
    "         # color='green',\n",
    "         label='Available'\n",
    "        )\n",
    "\n",
    "gdf.loc[index_train].plot(ax=ax, marker='.',\n",
    "                               color='orange',\n",
    "               label='Train'\n",
    "        )\n",
    "\n",
    "# gdf_train.plot(ax=ax, marker='.',\n",
    "#                                #  color='red',\n",
    "#                label='Train'\n",
    "#         )\n",
    "gdf.loc[index_val].plot(ax=ax, marker='.',\n",
    "color='blue',\n",
    "                label='Val'\n",
    "         )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "minx, miny, maxx, maxy = gdf.total_bounds\n",
    "margin_x = 2\n",
    "margin_y = 2\n",
    "\n",
    "\n",
    "\n",
    "ax.set_xlim(minx-margin_x, maxx+margin_x)\n",
    "ax.set_ylim(miny-margin_y, maxy+margin_y)\n",
    "\n",
    "\n",
    "\n",
    "gl = ax.gridlines(\n",
    "   crs=ccrs.PlateCarree(),\n",
    "    draw_labels=True) #x_inline=False, y_inline=False)\n",
    "gl.xformatter = LONGITUDE_FORMATTER\n",
    "gl.yformatter = LATITUDE_FORMATTER\n",
    "gl.xlabel_style = {'rotation':'vertical'}\n",
    "\n",
    "\n",
    "\n",
    "ax.legend(loc='lower left')\n",
    "# lgd = ax.legend(bbox_to_anchor=(1, 0.75), loc='upper left', ncol=1,\n",
    "#          title=\"Sample Type\")\n",
    "# plt.title('Fully labelled example \\n in Sumatra and Peninsular Malaysia \\n\\n\\n',)\n",
    "plt.savefig(save_path_figs+'fullylabelled_example.png', bbox_inches='tight',dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.split('manual_annotations')[-1] for x in tiles_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict = {'train':\n",
    "           [x.split('manual_annotations')[-1] for x in tiles_train],\n",
    "           'val':\n",
    "           [x.split('manual_annotations')[-1] for x in tiles_val]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#save_path = '/home/pf/pfstaff/projects/andresro/barry_palm/data/2A/datasets'\n",
    "#filename = save_path+'/palm4748a_base.json'\n",
    "#with open(filename, 'w') as fp:\n",
    "#    json.dump(out_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "save_path = '/home/pf/pfstaff/projects/andresro/barry_palm/data/2A/datasets'\n",
    "filename = save_path+'/palm4748a_base.json'\n",
    "with open(filename, 'rb') as fp:\n",
    "   out_dict = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tiles = {x.split('/')[1] for x in out_dict['val']}\n",
    "print(len(val_tiles))\n",
    "' '.join(val_tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_50 = ['/T47MQU/palm_group1_Bischel',\n",
    " '/T47NMD/palm_group3_Bischel',\n",
    " '/T47NQA/palm_group2',\n",
    " '/T47NNC/palm_group1_Brunner',\n",
    " '/T48MUC/palm_group1_Bischel',\n",
    " '/T47NPC/palm_group1_jacopo',\n",
    " '/T47NLE/palm_group3_hanlon',\n",
    " '/T48MUB/palm_group3_Brunner',\n",
    " '/T47NRA/palm_group3_Bischel',\n",
    " '/T48NUG/palm_group1',\n",
    " '/T47NRA/palm_group1_Bischel',\n",
    " '/T48MTC/palm_group1_Bischel',\n",
    " '/T47MQV/palm_group2',\n",
    " '/T47NPC/palm_group2_jacopo',\n",
    " '/T48MUA/palm_group1_Bischel',\n",
    " '/T48NTK/palm_group2',\n",
    " '/T47NNC/palm_group3_Bischel',\n",
    " '/T48MTE/palm_group1',\n",
    " '/T47NPA/palm_group1_Bischel',\n",
    " '/T47NMC/palm_group2_Bischel',\n",
    " '/T47NQB/palm_group1',\n",
    " '/T47MRV/palm_group1_siri',\n",
    " '/T47NMD/palm_group1_Bischel',\n",
    " '/T47NKF/palm_group1_bischel',\n",
    " '/T48MVB/palm_group3_Bischel',\n",
    " '/T48MWB/palm_group3_Bischel',\n",
    " '/T47NLD/palm_group1_florian',\n",
    " '/T48MUB/palm_group2_Brunner',\n",
    " '/T47NPC/palm_group3_jacopo',\n",
    " '/T48MUA/palm_group2_Bischel',\n",
    " '/T47NNB/palm_group3_stephane',\n",
    " '/T47NMC/palm_group1_Bischel',\n",
    " '/T47NLB/palm_group1_florian',\n",
    " '/T48MTC/palm_group3_Bischel',\n",
    " '/T47NLE/palm_group1_hanlon',\n",
    " '/T47NQD/palm_group1',\n",
    " '/T47NRF/palm_group1',\n",
    " '/T47NPA/palm_group3_Bischel',\n",
    " '/T48MUB/palm_group1_Brunner',\n",
    " '/T48MUB/palm_group3_Bischel',\n",
    " '/T47NRG/palm_group3',\n",
    " '/T47MQV/palm_group1',\n",
    " '/T47MQU/palm_group3_Bischel',\n",
    " '/T48MVB/palm_group1_Bischel',\n",
    " '/T48NUH/palm_group2',\n",
    " '/T47NRG/palm_group1',\n",
    " '/T47NKF/palm_group2_bischel',\n",
    " '/T47NMD/palm_group2_Bischel',\n",
    " '/T48MVV/palm_group3_matthew',\n",
    " '/T46NGL/palm_group2_janmathias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_50t = ['/T46NHL/palm_group4_bischel',\n",
    " '/T47MMU/palm_group1_julia',\n",
    " '/T47MMU/palm_group2_julia',\n",
    " '/T47MMU/palm_group3_julia',\n",
    " '/T47MQU/palm_group1_Bischel',\n",
    " '/T47MQV/palm_group2',\n",
    " '/T47MRS/palm_group2_Bischel',\n",
    " '/T47MRV/palm_group1_siri',\n",
    " '/T47NKF/palm_group1_bischel',\n",
    " '/T47NLB/palm_group2_florian',\n",
    " '/T47NLD/palm_group1_florian',\n",
    " '/T47NLE/palm_group3_hanlon',\n",
    " '/T47NMC/palm_group2_Bischel',\n",
    " '/T47NMD/palm_group1_Bischel',\n",
    " '/T47NMD/palm_group3_Bischel',\n",
    " '/T47NNC/palm_group1_Brunner',\n",
    " '/T47NNC/palm_group3_Bischel',\n",
    " '/T47NND/palm_group1_Brunner',\n",
    " '/T47NPA/palm_group1_Bischel',\n",
    " '/T47NPA/palm_group2_Bischel',\n",
    " '/T47NPB/palm_group1',\n",
    " '/T47NPC/palm_group1_jacopo',\n",
    " '/T47NPC/palm_group2_jacopo',\n",
    " '/T47NQA/palm_group2',\n",
    " '/T47NQB/palm_group1',\n",
    " '/T47NQD/palm_group1',\n",
    " '/T47NQD/palm_group2',\n",
    " '/T47NQE/palm_group1',\n",
    " '/T47NRA/palm_group1_Bischel',\n",
    " '/T47NRA/palm_group3_Bischel',\n",
    " '/T47NRF/palm_group1',\n",
    " '/T47NRF/palm_group3',\n",
    " '/T48MTC/palm_group1_Bischel',\n",
    " '/T48MTE/palm_group1',\n",
    " '/T48MUA/palm_group1_Bischel',\n",
    " '/T48MUB/palm_group3_Brunner',\n",
    " '/T48MUC/palm_group1_Bischel',\n",
    " '/T48MVB/palm_group3_Bischel',\n",
    " '/T48MVV/palm_group1_matthew',\n",
    " '/T48MVV/palm_group2_matthew',\n",
    " '/T48MWB/palm_group1_Bischel',\n",
    " '/T48MWB/palm_group2_Bischel',\n",
    " '/T48MWB/palm_group3_Bischel',\n",
    " '/T48MWV/palm_group1_Bischel',\n",
    " '/T48MWV/palm_group2_Bischel',\n",
    " '/T48MWV/palm_group3_Bischel',\n",
    " '/T48NTH/palm_group2',\n",
    " '/T48NTK/palm_group2',\n",
    " '/T48NUG/palm_group1',\n",
    " '/T48NUH/palm_group2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(out_dict['train']).intersection(set(opt_50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(opt_50t).intersection(set(opt_50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out val areas\n",
    "\n",
    "#tilenames_val = ['T47NQD', 'T48NUH'] # areas in peninsula\n",
    "#tilenames_val += ['T47NKF', 'T47MPV', 'T48MVB'] # areas in Sumatra\n",
    "\n",
    "#tilenames_tr = ['T47NRG','T47NPF','T47NRF','T47NQE','T47NRE']\n",
    "#tilenames_tr += ['T48NTK','T48NTJ','T48NTH','T48NUG']   \n",
    "\n",
    "\n",
    "#def isin(string_,list_):\n",
    "#    return np.any([x in string_ for x in list_])\n",
    "\n",
    "#tiles_train = [x for x in tiles_gt_all if isin(x,tilenames_tr)]\n",
    "#tiles_val = [x for x in tiles_gt_all if isin(x,tilenames_val)]\n",
    "\n",
    "#tiles_available = [x for x in tiles_gt_all if not isin(x,tilenames_val) and not isin(x,tilenames_tr)]\n",
    "\n",
    "#print(f'all {len(tiles_gt_all)}, train {len(tiles_train)}, val {len(tiles_val)}, remaining {len(tiles_available)}')\n",
    "\n",
    "#rois_available = np.array([gp.get_positive_area_folder(tile_) for tile_ in tiles_available])\n",
    "#rois_train = np.array([gp.get_positive_area_folder(tile_) for tile_ in tiles_train])\n",
    "#rois_val = np.array([gp.get_positive_area_folder(tile_) for tile_ in tiles_val])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def middle_roi(roi_list):\n",
    "#    return np.array([roi_list[:,0]+roi_list[:,2],roi_list[:,1]+roi_list[:,3]]).T/2\n",
    "\n",
    "#rois_middle = middle_roi(rois_available)\n",
    "#rois_middle_train = middle_roi(rois_train)\n",
    "#rois_middle_val = middle_roi(rois_val)\n",
    "\n",
    "#rois_middle_train = np.array([rois_train[:,0]+rois_train[:,2],rois_train[:,1]+rois_train[:,3]]).T/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading ensembe predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/scratch/andresro/leon_work/sparse/inference/palm4748a_simpleA9_soft_ens5'\n",
    "\n",
    "#path = '/scratch/andresro/leon_work/sparse/inference/palm4650_simpleA9_soft_mc5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tilenames = list({os.path.basename(x).split('_')[1] for x in glob.glob(path+'/R*')})\n",
    "len(tilenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every sample $i$ and dimension $j$:\n",
    "\n",
    "$$ \\sum_i ||X_i - Y ||^2_2 = \\sum_i \\big( \\sum_j X_{i,j}^2 - 2 \\sum_j (X_{i,j}*Y_j) +  \\sum_j Y_j^2 \\big)  $$\n",
    "\n",
    "$$ \\sum_j \\big( \\sum_i X_{i}^2 - 2 Y_j\\sum_i X_{i} + I* Y_j^2 \\big)  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate same locations\n",
    "for tilename in tilenames:\n",
    "    fname = f'{path}/{tilename}-5_preds_last.npz'\n",
    "    if not os.path.isfile(fname):\n",
    "        pred_dirs = glob.glob(f'{path}/*{tilename}/*.npz')\n",
    "\n",
    "        # sum over several days in same locations\n",
    "        x_sum1 = []\n",
    "        x2_sum1 = []\n",
    "        n1 = []\n",
    "\n",
    "        for dir_ in pred_dirs:\n",
    "            preds_last = np.load(dir_)\n",
    "\n",
    "            x_sum2, x2_sum2, n2 = preds_last['x_sum'], preds_last['x2_sum'], preds_last['n']\n",
    "            x_sum1.append(x_sum2)\n",
    "            x2_sum1.append(x2_sum2)\n",
    "            n1.append(n2)\n",
    "\n",
    "        x_sum1 = np.stack(x_sum1,axis=0).sum(axis=0)\n",
    "        x2_sum1 = np.stack(x2_sum1,axis=0).sum(axis=0)\n",
    "        n1 = np.stack(n1,axis=0).sum(axis=0)\n",
    "\n",
    "        np.savez(fname,x_sum=x_sum1,x2_sum=x2_sum1,\n",
    "                        n=n1, lonlat=preds_last['lonlat'])\n",
    "        print(fname,'saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_ = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sum = []\n",
    "x2_sum = []\n",
    "n = []\n",
    "lonlat = []\n",
    "std_patches=[]\n",
    "\n",
    "is_reload = False\n",
    "fname_all = f'{path}/all_statistics.npz'\n",
    "\n",
    "if not os.path.isfile(fname_all) or is_reload:\n",
    "    print('re-processing and saving')\n",
    "    \n",
    "    for tilename in tqdm(tilenames):\n",
    "\n",
    "        fname = f'{path}/{tilename}-5_preds_last.npz'\n",
    "        if os.path.isfile(fname):\n",
    "            preds_last = np.load(fname)\n",
    "            x_sum0, x2_sum0, n0, lonlat0 = preds_last['x_sum'], preds_last['x2_sum'], preds_last['n'], preds_last['lonlat']\n",
    "            \n",
    "            \n",
    "            # Adding nan values to patch statistics\n",
    "            down100_file = f'{path}/{tilename}_5_preds_reg_{compression_}_{compression_}_down100.vrt'\n",
    "            ds = gdal.Open(down100_file)\n",
    "            down_array = ds.ReadAsArray()\n",
    "            is_nan_array = interpPatches((down_array==99), ref_shape=(100,100)).squeeze() > 0.5\n",
    "            is_nan_array = is_nan_array.reshape(-1)\n",
    "\n",
    "            x_sum0[is_nan_array] = np.nan\n",
    "            x2_sum0[is_nan_array] = np.nan\n",
    "            n0[is_nan_array] = 0\n",
    "            \n",
    "            x_sum.append(x_sum0)\n",
    "            x2_sum.append(x2_sum0)\n",
    "            n.append(n0)\n",
    "            lonlat.append(lonlat0)\n",
    "\n",
    "            # Adding mean uncertainty per patch\n",
    "            std_file = f'{path}/std/{tilename}_5_preds_reg_{compression_}_std_{compression_}.tif'\n",
    "            ds = gdal.Open(std_file)\n",
    "            std_array = ds.ReadAsArray()\n",
    "            std_array[std_array == 99] = np.nan\n",
    "            std_patch = block_reduce(std_array,(110,110),np.mean)\n",
    "            std_patches.append(std_patch.reshape(-1))\n",
    "\n",
    "    x_sum = np.stack(x_sum,axis=0)\n",
    "    x2_sum = np.stack(x2_sum,axis=0)\n",
    "    n = np.stack(n,axis=0)\n",
    "    lonlat = np.stack(lonlat,axis=0)\n",
    "    std_patches = np.stack(std_patches,axis=0)\n",
    "\n",
    "    np.savez(fname_all,x_sum=x_sum,\n",
    "            x2_sum=x2_sum,\n",
    "            n=n, lonlat=lonlat,\n",
    "            std_patches = std_patches)    \n",
    "else:\n",
    "    print('reading',fname_all)\n",
    "    preds_all = np.load(fname_all)\n",
    "\n",
    "    x_sum = preds_all['x_sum']\n",
    "    x2_sum = preds_all['x2_sum']\n",
    "    n = preds_all['n']\n",
    "    lonlat = preds_all['lonlat']\n",
    "    std_patches = preds_all['std_patches']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#is_nan_patch = np.isnan(x_sum[...,0])\n",
    "is_nan_patch = n == 0\n",
    "#is_nan_patch = np.logical_or(n == 0,np.isnan(std_patches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_nan_patch = np.zeros_like(n) != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_nan_patch.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_nan_patch.shape,std_patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(lonlat[is_nan_patch][...,0], lonlat[is_nan_patch][...,1],\n",
    "            #c=np.log(weight_sorted[:top_k]),\n",
    "            # c=weight_sorted[:top_k],\n",
    "            cmap='magma_r', marker='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(lonlat[~is_nan_patch][...,0], lonlat[~is_nan_patch][...,1],\n",
    "            #c=np.log(weight_sorted[:top_k]),\n",
    "            # c=weight_sorted[:top_k],\n",
    "            cmap='magma_r', marker='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sum[~is_nan_patch].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_global = np.sum(x_sum[~is_nan_patch], axis=0) / n.sum()\n",
    "\n",
    "# error means quantization error not std. dev.\n",
    "error_patch = (x2_sum[~is_nan_patch] - 2*x_sum[~is_nan_patch]*mean_global + n.sum() * (mean_global**2))\n",
    "\n",
    "# error_patch = np.clip(error_patch,0,np.percentile(error_patch,99))\n",
    "\n",
    "error_global = np.sum(error_patch, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_global.shape, error_patch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_min = 0\n",
    "p_max = 99\n",
    "\n",
    "uncertainty = np.nan_to_num(std_patches[~is_nan_patch][...,np.newaxis])\n",
    "\n",
    "weight_div = error_patch.sum(axis=-1).reshape(-1) / np.sum(error_patch)\n",
    "\n",
    "weight_div = np.clip(weight_div,np.percentile(weight_div,p_min),np.percentile(weight_div,p_max))\n",
    "weight_div = weight_div / weight_div.sum()\n",
    "\n",
    "weight_unc = (uncertainty).sum(axis=-1).reshape(-1) / np.sum(uncertainty)\n",
    "\n",
    "#weight_unc = np.clip(weight_unc,np.percentile(weight_unc,p_min),np.percentile(weight_unc,p_max))\n",
    "#weight_unc = weight_unc / weight_unc.sum()\n",
    "\n",
    "\n",
    "weight_score = (weight_div + weight_unc)/2.0\n",
    "weight_score = weight_score/weight_score.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_perc = 99\n",
    "\n",
    "f, ax = plt.subplots(2,2, figsize=(10,5))\n",
    "ax[0,0].plot(weight_div,'x')\n",
    "ax[0,1].plot(weight_unc,'x')\n",
    "\n",
    "_ = ax[1,0].hist(np.clip(weight_div,0,np.percentile(weight_div,max_perc)))\n",
    "_ = ax[1,1].hist(np.clip(weight_unc,0,np.percentile(weight_unc,max_perc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(weight_div,90), np.percentile(weight_div,99), np.percentile(weight_div,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index = np.random.choice(weight.shape[0],100000)\n",
    "p_min = 0\n",
    "p_max = 100\n",
    "\n",
    "index_inlier = np.logical_and.reduce(\n",
    "                      (weight_div > np.percentile(weight_div,p_min),\n",
    "                       weight_div < np.percentile(weight_div,p_max),\n",
    "                       weight_unc > np.percentile(weight_unc,p_min),\n",
    "                       weight_unc < np.percentile(weight_unc,p_max),\n",
    "                      ))\n",
    "\n",
    "\n",
    "g = sns.jointplot(x=weight_div[index_inlier], y=weight_unc[index_inlier], kind=\"hex\",\n",
    "                #  xlim=(np.percentile(weight,p_min),np.percentile(weight,p_max)),\n",
    "                # ylim=(np.percentile(weight1,p_min),np.percentile(weight1,100))) #, data=tips))\n",
    "                 )\n",
    "\n",
    "g.ax_joint.set_xlabel('Quant Error')\n",
    "g.ax_joint.set_ylabel('Uncertainty')\n",
    "#g.ax_joint.set_xlim(0,1)\n",
    "\n",
    "\n",
    "# g.ax_joint.set_xlim(np.percentile(weight_div,p_min),np.percentile(weight_div,80))\n",
    "#g.ax_joint.set_ylim(np.percentile(weight1,p_min),np.percentile(weight1,100))\n",
    "# g.ax_joint.set_xticks([])\n",
    "# g.ax_joint.set_yticks([])\n",
    "\n",
    "\n",
    "\n",
    "# plt.savefig(save_path_figs + 'uncertainty_vs_quant_palm4748.png', bbox_inches='tight', dpi= 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_sorted = np.argsort(weight_score)[::-1] \n",
    "\n",
    "weight_sorted = weight_score[ids_sorted]\n",
    "lonlat_sorted = lonlat[~is_nan_patch][ids_sorted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_train = [x.split('manual_annotations')[-1] in out_dict['train'] for x in tiles_gt_all]\n",
    "# index_val = [x.split('manual_annotations')[-1] in out_dict['val'] for x in tiles_gt_all]\n",
    "\n",
    "# index_bool = [x.split('manual_annotations')[-1] in out_dict['30opt'] for x in tiles_gt_all]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(index_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lonlat_sorted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rois_middle[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rois_middle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_active = 30\n",
    "\n",
    "gdf = geopandas.GeoDataFrame(\n",
    "    geometry=geopandas.points_from_xy(rois_middle[:,0], rois_middle[:,1]))\n",
    "\n",
    "gdf = geopandas.GeoDataFrame(\n",
    "    geometry=geopandas.points_from_xy(rois_middle[:,0], rois_middle[:,1]))\n",
    "\n",
    "\n",
    "gdf_active = geopandas.GeoDataFrame(\n",
    "    geometry=geopandas.points_from_xy(lonlat_sorted[:N_active,0], lonlat_sorted[:N_active,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 1000\n",
    "plt.scatter(lonlat_sorted[:top_k,0], lonlat_sorted[:top_k,1],\n",
    "            #c=np.log(weight_sorted[:top_k]),\n",
    "            c=weight_sorted[:top_k],\n",
    "            cmap='magma_r', marker='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))\n",
    "countries = geopandas.read_file(\"/scratch/andresro/leon_work/barry_palm/data/labels/countries/3countries.shp\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "\n",
    "world.plot(ax=ax,\n",
    "    color='lightgray', edgecolor='white')\n",
    "\n",
    "#countries.plot(ax=ax,\n",
    "#    color='lightgray', edgecolor='white')\n",
    "\n",
    "gdf.loc[index_train].plot(ax=ax, marker='.',\n",
    "                               color='orange',\n",
    "               label='Train'\n",
    "        )\n",
    "\n",
    "gdf.loc[index_val].plot(ax=ax, marker='.',\n",
    "color='blue',\n",
    "                label='Val'\n",
    "         )\n",
    "\n",
    "gdf_active.plot(ax=ax, marker='.',\n",
    "                                 color='red', \n",
    "                                 label='Active'\n",
    "        )\n",
    "\n",
    "\n",
    "minx, miny, maxx, maxy = gdf.total_bounds\n",
    "margin_x = 2\n",
    "margin_y = 2\n",
    "\n",
    "\n",
    "minx1, miny1, maxx1, maxy1 = gdf_active.total_bounds\n",
    "\n",
    "minx = min(minx,minx1)\n",
    "miny = min(miny,miny1)\n",
    "maxx = max(maxx,maxx1)\n",
    "maxy = max(maxy,maxy1)\n",
    "\n",
    "\n",
    "ax.set_xlim(minx-margin_x, maxx+margin_x)\n",
    "ax.set_ylim(miny-margin_y, maxy+margin_y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "gl = ax.gridlines(\n",
    "   crs=ccrs.PlateCarree(),\n",
    "    draw_labels=True) #x_inline=False, y_inline=False)\n",
    "gl.xformatter = LONGITUDE_FORMATTER\n",
    "gl.yformatter = LATITUDE_FORMATTER\n",
    "gl.xlabel_style = {'rotation':'vertical'}\n",
    "\n",
    "\n",
    "# ax.legend(bbox_to_anchor=(1, 0.75), loc='upper left', ncol=1,\n",
    "#          title=\"Sample Type\")\n",
    "#ax.legend() # loc='lower left')\n",
    "\n",
    "# plt.title('Active Learning Selected Samples\\n\\n')\n",
    "# plt.savefig(save_path_figs + 'active_learning_samples_10opt.', bbox_inches='tight', dpi= 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simplekml\n",
    "\n",
    "kml = simplekml.Kml()\n",
    "for key, val in enumerate(lonlat_sorted[:N_active]):\n",
    "    #lat, lon = gp.to_latlon(val[1],val[0],ds_pred)\n",
    "    lon, lat = val\n",
    "    pnt = kml.newpoint(name=str(key), coords=[(lon, lat)])\n",
    "    \n",
    "#Â kml.save(f'/scratch2/Dropbox/Dropbox/0_phd/temp/{N_active}_New_samples_palm4.kml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### looking for the closest patch for each labeled area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_node(node, nodes):\n",
    "    dist_2 = np.sum((nodes - node)**2, axis=-1)\n",
    "    min_index = np.argmin(dist_2)\n",
    "    dist_min = dist_2[min_index]\n",
    "    return min_index, dist_min\n",
    "#     return np.divmod(min_index,dist_2.shape[-1]),dist_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_id_avail = []\n",
    "distances = []\n",
    "for roi in rois_middle[index_avail]:\n",
    "\n",
    "    node_id, distance = closest_node(roi,lonlat[~is_nan_patch])\n",
    "    patch_id_avail.append(node_id)\n",
    "    distances.append(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(lonlat[~is_nan_patch][patch_id_avail][:,0], lonlat[~is_nan_patch][patch_id_avail][:,1],\n",
    "# plt.scatter(lonlat[~is_nan_patch][core_set][patch_id_gt][:,0], lonlat[~is_nan_patch][core_set][patch_id_gt][:,1],            \n",
    "# plt.scatter(lonlat.reshape(-1,2)[patch_id_gt][:,0], lonlat.reshape(-1,2)[patch_id_gt][:,1],\n",
    "            #c=np.log(weight_sorted[:top_k]),\n",
    "#             c=weight_sorted[:top_k],\n",
    "#             cmap='magma_r',\n",
    "            marker='o')\n",
    "plt.scatter(rois_middle[:,0], rois_middle[:,1],\n",
    "            #c=np.log(weight_sorted[:top_k]),\n",
    "#             c=weight_sorted[:top_k],\n",
    "#             cmap='magma_r',\n",
    "            marker='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using top-k from each weight independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_unc[patch_id_avail].shape,index_avail.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "C=50\n",
    "\n",
    "\n",
    "core_set = set()\n",
    "\n",
    "unc_sorted = np.argsort(weight_unc[patch_id_avail])[::-1]\n",
    "div_sorted = np.argsort(weight_div[patch_id_avail])[::-1]\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "while len(core_set) < C:\n",
    "    \n",
    "    if np.mod(i+j,2) == 0:\n",
    "        core_set.add(unc_sorted[i])\n",
    "        i+=1\n",
    "    else:\n",
    "        core_set.add(div_sorted[j])\n",
    "        j+=1\n",
    "        \n",
    "core_set = np.array(list(core_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_set_score = np.argsort(weight_score[patch_id_avail])[::-1][:C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(core_set).intersection(set(core_set_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.jointplot(x=weight_div, y=weight_unc, kind=\"hex\")\n",
    "\n",
    "g.ax_joint.set_xlabel('Quant Error')\n",
    "g.ax_joint.set_ylabel('Uncertainty')\n",
    "# g.ax_joint.set_xlim(np.percentile(weight_div,p_min),np.percentile(weight_div,80))\n",
    "#g.ax_joint.set_ylim(np.percentile(weight1,p_min),np.percentile(weight1,100))\n",
    "# g.ax_joint.set_xticks([])\n",
    "# g.ax_joint.set_yticks([])\n",
    "g.ax_joint.scatter(x=weight_div[patch_id_avail],\n",
    "                   y=weight_unc[patch_id_avail], c='green')\n",
    "#g.ax_joint.scatter(x=weight_div[patch_id_avail][core_set],\n",
    "#                   y=weight_unc[patch_id_avail][core_set], c='white', marker='+')\n",
    "g.ax_joint.scatter(x=weight_div[patch_id_avail][core_set_score],\n",
    "                   y=weight_unc[patch_id_avail][core_set_score],\n",
    "                   marker='o',\n",
    "                   edgecolor='red',\n",
    "                   facecolor='none',\n",
    "                   )\n",
    "\n",
    "# plt.savefig(save_path_figs + 'uncertainty_vs_quant_all.png', bbox_inches='tight', dpi= 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.jointplot(x=weight_div, y=weight_unc, kind=\"hex\")\n",
    "\n",
    "g.ax_joint.set_xlabel('Quant Error')\n",
    "g.ax_joint.set_ylabel('Uncertainty')\n",
    "# g.ax_joint.set_xlim(np.percentile(weight_div,p_min),np.percentile(weight_div,80))\n",
    "#g.ax_joint.set_ylim(np.percentile(weight1,p_min),np.percentile(weight1,100))\n",
    "# g.ax_joint.set_xticks([])\n",
    "# g.ax_joint.set_yticks([])\n",
    "g.ax_joint.scatter(x=weight_div[patch_id_avail], y=weight_unc[patch_id_avail], c='green')\n",
    "g.ax_joint.scatter(x=weight_div[patch_id_avail][core_set], y=weight_unc[patch_id_avail][core_set], c='red', marker='x')\n",
    "\n",
    "# plt.savefig(save_path_figs + 'uncertainty_vs_quant_all.png', bbox_inches='tight', dpi= 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict = {}\n",
    "M_list = [5, 10, 15, 30, 50]\n",
    "\n",
    "for M in M_list:\n",
    "    core_set = set()\n",
    "\n",
    "    unc_sorted = np.argsort(weight_unc[patch_id_avail])[::-1]\n",
    "    div_sorted = np.argsort(weight_div[patch_id_avail])[::-1]\n",
    "\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while len(core_set) < M:\n",
    "\n",
    "        if np.mod(i+j,2) == 0:\n",
    "            core_set.add(unc_sorted[i])\n",
    "            i+=1\n",
    "        else:\n",
    "            core_set.add(div_sorted[j])\n",
    "            j+=1\n",
    "\n",
    "    core_set = np.array(list(core_set))\n",
    "    \n",
    "    selected_files = sorted(tiles_avail[core_set])\n",
    "    key_ = f'{M}optt'\n",
    "    out_dict[key_] = [x.split('manual_annotations')[-1] for x in selected_files]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "save_path = '/home/pf/pfstaff/projects/andresro/barry_palm/data/2A/datasets'\n",
    "filename = save_path+'/palm4748a_activesamples_topk.json'\n",
    "with open(filename, 'w') as fp:\n",
    "    json.dump(out_dict, fp)\n",
    "\n",
    "#with open(filename, 'r') as fp:\n",
    "#    out_dict = json.load(fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using core-set for a k-mean solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample C samples to create a core-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_avail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 10000\n",
    "N_samples = weight_score.shape[0]\n",
    "\n",
    "# w_ = weight_score[n[~is_nan_patch] > 0]/weight_score[n[~is_nan_patch] > 0].sum()\n",
    "# w_ = weight_score[n.reshape(-1) > 0]/weight_score[n.reshape(-1) > 0].sum()\n",
    "# w_ = weight_score[np.logical_and(n.reshape(-1) > 0,np.percentile(weight_score,99))]/weight_score[np.logical_and(n.reshape(-1) > 0,np.percentile(weight_score,99))].sum()\n",
    "# w_ = weight_unc[n.reshape(-1) > 0]/weight_unc[n.reshape(-1) > 0].sum()\n",
    "\n",
    "samples_id = np.arange(N_samples) # [n[~is_nan_patch] > 0]\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "core_set = np.random.choice(samples_id,size=C, replace=True, p=weight_score)\n",
    "\n",
    "# adding the labeled samples to the coreset\n",
    "core_set = np.concatenate((core_set,patch_id_gt))\n",
    "\n",
    "# ids_sorted = np.argsort(weight_score)[::-1] \n",
    "\n",
    "# core_set = samples_id[ids_sorted][:C]\n",
    "\n",
    "core_set_weight = 1 / (C* weight_score[core_set])\n",
    "# core_set_weight = 1 / (C * weight_unc[core_set])\n",
    "\n",
    "# key_ = f'{M}{rand_option}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.jointplot(x=weight_div[core_set], y=weight_unc[core_set], kind=\"hex\")\n",
    "\n",
    "g.ax_joint.set_xlabel('Quant Error')\n",
    "g.ax_joint.set_ylabel('Uncertainty')\n",
    "# g.ax_joint.set_xlim(np.percentile(weight_div,p_min),np.percentile(weight_div,80))\n",
    "#g.ax_joint.set_ylim(np.percentile(weight1,p_min),np.percentile(weight1,100))\n",
    "# g.ax_joint.set_xticks([])\n",
    "# g.ax_joint.set_yticks([])\n",
    "g.ax_joint.scatter(x=weight_div[patch_id_gt], y=weight_unc[patch_id_gt], c='red')\n",
    "\n",
    "# plt.savefig(save_path_figs + 'uncertainty_vs_quant_all.png', bbox_inches='tight', dpi= 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean_coreset = x_sum[~is_nan_patch][core_set] / (n[~is_nan_patch][core_set][...,np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lonlat.reshape(-1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patch_id_gt = []\n",
    "# distances = []\n",
    "# for roi in rois_middle[index_avail]:\n",
    "\n",
    "#     node_id, distance = closest_node(roi,lonlat[~is_nan_patch][core_set])\n",
    "#     patch_id_gt.append(node_id)\n",
    "#     distances.append(distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "Y = TSNE(n_components=2).fit_transform(x_mean_coreset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.jointplot(Y[:,0],Y[:,1],kind='kde')\n",
    "\n",
    "# sns.jointplot(Y[patch_id_gt][:,0],Y[patch_id_gt][:,1])\n",
    "g.ax_joint.scatter(Y[-len(patch_id_gt):][:,0],Y[-len(patch_id_gt):][:,1], c='red')\n",
    "# g.ax_joint.scatter(Y[patch_id_gt][center_pixels_index][:,0],Y[patch_id_gt][center_pixels_index][:,1], c='white', marker='.', facecolor=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error_patch1 = np.array([error_patch[id_] for id_ in patch_id_gt])\n",
    "# std_patch1 = np.array([std_patches[id_] for id_ in patch_id_gt])\n",
    "\n",
    "x_mean_labeled = x_sum[~is_nan_patch][patch_id_gt]/ (n[~is_nan_patch][patch_id_gt][...,np.newaxis])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncertainty1 = np.nan_to_num(std_patch1[...,np.newaxis])\n",
    "# weight = error_patch1.sum(axis=-1).reshape(-1) / error_patch1.sum()\n",
    "# weight1 = (uncertainty1).sum(axis=-1).reshape(-1) / (uncertainty1).sum()\n",
    "\n",
    "# weight2 = (weight + weight1)/2.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 30\n",
    "\n",
    "model = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "model.fit(x_mean_coreset[-len(patch_id_gt):], sample_weight=core_set_weight[-len(patch_id_gt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id = model.predict(x_mean_coreset)\n",
    "center_pixels_index_coreset = np.argmin(model.transform(x_mean_coreset), axis=0)\n",
    "\n",
    "\n",
    "center_pixels_index = np.argmin(model.transform(x_mean_labeled), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_min_weight = np.argsort(weight_score[patch_id_gt])[::-1][:n_clusters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Y[:,0],Y[:,1], c=cluster_id,cmap='tab20')\n",
    "\n",
    "\n",
    "plt.scatter(Y[center_pixels_index_coreset][:,0],Y[center_pixels_index_coreset][:,1],color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.jointplot(Y[:,0],Y[:,1],kind='kde')\n",
    "\n",
    "# sns.jointplot(Y[patch_id_gt][:,0],Y[patch_id_gt][:,1])\n",
    "g.ax_joint.scatter(Y[-len(patch_id_gt):][:,0],Y[-len(patch_id_gt):][:,1], c='red')\n",
    "g.ax_joint.scatter(Y[-len(patch_id_gt):][center_pixels_index][:,0],Y[-len(patch_id_gt):][center_pixels_index][:,1],  c='white', marker='.', facecolor=None)\n",
    "\n",
    "g.ax_joint.scatter(Y[-len(patch_id_gt):][id_min_weight][:,0],Y[-len(patch_id_gt):][id_min_weight][:,1],  c='green', marker='.', facecolor=None)\n",
    "# g.ax_joint.scatter(Y[patch_id_gt][center_pixels_index][:,0],Y[patch_id_gt][center_pixels_index][:,1], c='white', marker='.', facecolor=None)\n",
    "# g.ax_joint.scatter(Y[center_pixels_index_coreset][:,0],Y[center_pixels_index_coreset][:,1], c='green', marker='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(center_pixels_index)), center_pixels_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_id = model.predict(x_mean_coreset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.jointplot(x=weight_div[core_set], y=weight_unc[core_set], kind=\"hex\")\n",
    "\n",
    "\n",
    "g.ax_joint.set_xlabel('Quant Error')\n",
    "g.ax_joint.set_ylabel('Uncertainty')\n",
    "# g.ax_joint.set_xlim(np.percentile(weight_div,p_min),np.percentile(weight_div,80))\n",
    "#g.ax_joint.set_ylim(np.percentile(weight1,p_min),np.percentile(weight1,100))\n",
    "# g.ax_joint.set_xticks([])\n",
    "# g.ax_joint.set_yticks([])\n",
    "# g.ax_joint.scatter(x=weight_div[patch_id_gt][center_pixels_index],\n",
    "#                    y=weight_unc[patch_id_gt][center_pixels_index], c='red')\n",
    "g.ax_joint.scatter(x=weight_div[core_set][-len(patch_id_gt):][center_pixels_index],\n",
    "                   y=weight_unc[core_set][-len(patch_id_gt):][center_pixels_index],c='red')\n",
    "g.ax_joint.scatter(x=weight_div[core_set][-len(patch_id_gt):][id_min_weight],\n",
    "                   y=weight_unc[core_set][-len(patch_id_gt):][id_min_weight],c='green')\n",
    "\n",
    "# plt.savefig(save_path_figs + 'uncertainty_vs_quant_coreset.png', bbox_inches='tight', dpi= 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_coreset = geopandas.GeoDataFrame(\n",
    "    geometry=geopandas.points_from_xy(coreset_lonlat_coords[:,0], coreset_lonlat_coords[:,1]))\n",
    "\n",
    "gdf_active = geopandas.GeoDataFrame(\n",
    "    geometry=geopandas.points_from_xy(rois_middle[id_min_weight][:,0], rois_middle[id_min_weight][:,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_pixels_index_coreset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))\n",
    "#countries = geopandas.read_file(\"/scratch/andresro/leon_work/barry_palm/data/labels/countries/3countries.shp\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "\n",
    "# gdf_coreset.plot(ax=ax, marker='o',\n",
    "#                                  color='lightgray', \n",
    "#                                  label='Coreset'\n",
    "#         )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "world.plot(ax=ax,\n",
    "    color='lightgray', edgecolor='white')\n",
    "\n",
    "# countries.plot(ax=ax,\n",
    "#     color='lightgray', edgecolor='white')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "gdf.loc[index_train].plot(ax=ax, marker='.',\n",
    "                               color='orange',\n",
    "               label='Train'\n",
    "        )\n",
    "\n",
    "gdf.loc[index_val].plot(ax=ax, marker='.',\n",
    "color='blue',\n",
    "                label='Val'\n",
    "         )\n",
    "\n",
    "\n",
    "gdf.loc[index_avail].plot(ax=ax,legend=True, marker='.',\n",
    "          edgecolor='black',\n",
    "          facecolor='none',\n",
    "         # color='green',\n",
    "         label='Available'\n",
    "        )\n",
    "\n",
    "\n",
    "gdf_active.plot(ax=ax, marker='.',\n",
    "                                 color='red', \n",
    "                                 label='Active'\n",
    "        )\n",
    "\n",
    "gdf_coreset.loc[center_pixels_index_coreset].plot(ax=ax, marker='x',\n",
    "                                 color='green', \n",
    "                                 label='Coreset'\n",
    "        )\n",
    "minx, miny, maxx, maxy = gdf.total_bounds\n",
    "margin_x = 2\n",
    "margin_y = 2\n",
    "\n",
    "\n",
    "minx1, miny1, maxx1, maxy1 = gdf_coreset.total_bounds\n",
    "\n",
    "minx = min(minx,minx1)\n",
    "miny = min(miny,miny1)\n",
    "maxx = max(maxx,maxx1)\n",
    "maxy = max(maxy,maxy1)\n",
    "\n",
    "\n",
    "ax.set_xlim(minx-margin_x, maxx+margin_x)\n",
    "ax.set_ylim(miny-margin_y, maxy+margin_y)\n",
    "\n",
    "\n",
    "gl = ax.gridlines(\n",
    "   crs=ccrs.PlateCarree(),\n",
    "    draw_labels=True) #x_inline=False, y_inline=False)\n",
    "gl.xformatter = LONGITUDE_FORMATTER\n",
    "gl.yformatter = LATITUDE_FORMATTER\n",
    "gl.xlabel_style = {'rotation':'vertical'}\n",
    "\n",
    "\n",
    "# ax.legend(bbox_to_anchor=(1, 0.75), loc='upper left', ncol=1,\n",
    "#          title=\"Sample Type\")\n",
    "ax.legend() # loc='lower left')\n",
    "\n",
    "# plt.title('Active Learning Selected Samples\\n\\n')\n",
    "# \n",
    "\n",
    "# plt.savefig(save_path_figs + 'palm4_30_ALsamples.pdf', bbox_inches='tight', dpi= 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simplekml\n",
    "\n",
    "kml = simplekml.Kml()\n",
    "for key, val in enumerate(center_lonlat_coords):\n",
    "    #lat, lon = gp.to_latlon(val[1],val[0],ds_pred)\n",
    "    lon, lat = val\n",
    "    pnt = kml.newpoint(name=str(key), coords=[(lon, lat)])\n",
    "    \n",
    "# kml.save(f'/scratch2/Dropbox/Dropbox/0_phd/temp/{n_clusters}samples_coreset100k_combinedscore.kml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict = {}\n",
    "M_list = [10, 15, 30, 50, 5]\n",
    "\n",
    "for M in M_list:\n",
    "    \n",
    "    model = KMeans(n_clusters=M, random_state=0)\n",
    "    model.fit(x_mean_coreset, sample_weight=core_set_weight)\n",
    "\n",
    "    cluster_id = model.predict(x_mean_coreset)\n",
    "\n",
    "    center_pixels_index = np.argmin(model.transform(x_mean_labeled), axis=0)\n",
    "    \n",
    "    \n",
    "    selected_files = tiles_avail[center_pixels_index]\n",
    "    key_ = f'{M}optk'\n",
    "    out_dict[key_] = [x.split('manual_annotations')[-1] for x in selected_files]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "save_path = '/home/pf/pfstaff/projects/andresro/barry_palm/data/2A/datasets'\n",
    "filename = save_path+'/palm4748a_activesamples_kmeans.json'\n",
    "with open(filename, 'w') as fp:\n",
    "    json.dump(out_dict, fp)\n",
    "\n",
    "# with open(filename, 'r') as fp:\n",
    "#    out_dict = json.load(fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random and Manual Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_node(node, nodes):\n",
    "    dist_2 = np.sum((nodes - node)**2, axis=-1)\n",
    "    min_index = np.argmin(dist_2)\n",
    "    dist_min = dist_2.reshape(-1)[min_index]\n",
    "    return np.divmod(min_index,dist_2.shape[-1]),dist_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_id_gt = []\n",
    "distances = []\n",
    "for roi in rois_middle:\n",
    "\n",
    "    node_id, distance = closest_node(roi,lonlat[~is_nan_patch])    \n",
    "    patch_id_gt.append(node_id)\n",
    "    distances.append(distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_patch1 = np.array([error_patch[id_] for id_ in patch_id_gt])\n",
    "std_patch1 = np.array([std_patches[id_] for id_ in patch_id_gt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty1 = np.nan_to_num(std_patch1[...,np.newaxis])\n",
    "weight = error_patch1.sum(axis=-1).reshape(-1) / error_patch1.sum()\n",
    "weight1 = (uncertainty1).sum(axis=-1).reshape(-1) / (uncertainty1).sum()\n",
    "\n",
    "weight2 = (weight + weight1)/2.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2,2, figsize=(10,5))\n",
    "ax[0,0].plot(weight,'x')\n",
    "ax[0,1].plot(weight1,'x')\n",
    "\n",
    "max_perc = 100\n",
    "_ = ax[1,0].hist(np.clip(weight,0,np.percentile(weight,max_perc)))\n",
    "_ = ax[1,1].hist(np.clip(weight1,0,np.percentile(weight1,max_perc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_inlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "#index = np.random.choice(weight.shape[0],100000)\n",
    "p_min = 0\n",
    "p_max = 99\n",
    "\n",
    "\n",
    "index_inlier = np.logical_and.reduce((weight >= np.percentile(weight,p_min),\n",
    "                       weight <= np.percentile(weight,p_max),\n",
    "                       weight1 >= np.percentile(weight1,p_min),\n",
    "                     #                 weight1 > 0,\n",
    "                       weight1 <= np.percentile(weight1,p_max),\n",
    "                      ))\n",
    "\n",
    "\n",
    "g = sns.jointplot(x=weight[index_inlier], y=weight1[index_inlier], kind=\"hex\",\n",
    "                #  xlim=(np.percentile(weight,p_min),np.percentile(weight,p_max)),\n",
    "                # ylim=(np.percentile(weight1,p_min),np.percentile(weight1,100))) #, data=tips))\n",
    "                 )\n",
    "\n",
    "_ = g.ax_joint.set_xlabel('Quant Error')\n",
    "_ = g.ax_joint.set_ylabel('Uncertainty')\n",
    "#g.ax_joint.set_xlim(0,1)\n",
    "\n",
    "#g.ax_joint.set_xlim(np.percentile(weight,p_min),np.percentile(weight,p_max))\n",
    "#g.ax_joint.set_ylim(np.percentile(weight1,p_min),np.percentile(weight1,100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "','.join({x.split('/')[8] for x in tiles_gt_all})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Creating active learning datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_sample = weight2[index_avail]\n",
    "\n",
    "weights_sample=  weights_sample / weights_sample.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(weight[index_inlier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(weight1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.sort(weights_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(tiles_avail) ==  weights_sample.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing areas randomdummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomdummy_groups = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_ = '47NRG,47NRF,47NPF,47NQE,47NRE,48NTK,47NQD,47NRD,48NTJ,47NRC,48NTH,48NUG,48NUH'.split(',')\n",
    "\n",
    "group_ = [x.split('manual_annotations')[-1] for x in tiles_avail1 if isin(x,group_)]\n",
    "\n",
    "randomdummy_groups['A'] = group_\n",
    "\n",
    "#index_  = [list(tiles_gt_all).index(path_+x) for x in groupA]\n",
    "print(len(group_))\n",
    "#index_bool = [x in index_ for x in list(gdf.index)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_ = '47NRG,47NRF,47NPF,47NQE,47NRE,48NTK,46NGM,46NGL,46NHL,47NKF,47NLE,47NLD,47NNE'.split(',')\n",
    "\n",
    "group_ = [x.split('manual_annotations')[-1] for x in tiles_avail1 if isin(x,group_)]\n",
    "\n",
    "randomdummy_groups['B'] = group_\n",
    "print(len(group_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_ = '46NGM,46NGL,46NHL,47NKF,47NLE,47NLD,47NNE,47NLD,47NMD,47NND,47NNC,47NMC'.split(',')\n",
    "\n",
    "group_ = [x.split('manual_annotations')[-1] for x in tiles_avail1 if isin(x,group_)]\n",
    "randomdummy_groups['C'] = group_\n",
    "print(len(group_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_ = '47NNC,47NMC,47NPC,47NLB,47NMB,47NNB,47NPB,47NQB,47NNA,47NPA,47NQA,47NRA'.split(',')\n",
    "\n",
    "group_ = [x.split('manual_annotations')[-1] for x in tiles_avail1 if isin(x,group_)]\n",
    "randomdummy_groups['D'] = group_\n",
    "print(len(group_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_ = '47NQA,47NRA,47MPV,47MQV,47MRV,48MTE,47MQU,47MRU,48MTD,47MRT,48MUC,47MRS,48MUB,48MVB,48MTC'.split(',')\n",
    "\n",
    "group_ = [x.split('manual_annotations')[-1] for x in tiles_avail1 if isin(x,group_)]\n",
    "randomdummy_groups['E'] = group_\n",
    "print(len(group_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_avail1 = np.concatenate((tiles_avail, tiles_train),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_ = '48MUB,48MVB,48MWC,48MWB,48MUA,48MVV,48MWV,48MVU,47MRS'.split(',')\n",
    "\n",
    "group_ = [x.split('manual_annotations')[-1] for x in tiles_avail1 if isin(x,group_)]\n",
    "\n",
    "randomdummy_groups['F'] = group_\n",
    "print(len(group_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_dummy = sorted(list(randomdummy_groups.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_ = np.random.choice(keys_dummy,replace=False,size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tiles_list(dummy_order):\n",
    "    tiles_ = []\n",
    "    for o in dummy_order:\n",
    "        tiles_.append(np.random.choice(randomdummy_groups[o],replace=False,size=len(randomdummy_groups[o])))\n",
    "    tiles_ = np.concatenate(tiles_)\n",
    "    _, idx = np.unique(tiles_, return_index=True)\n",
    "    return tiles_[np.sort(idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_tiles_list(order_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weight = error_patch1.sum(axis=-1).reshape(-1) / error_patch1.sum()\n",
    "\n",
    "nareas, d  = error_patch1.shape \n",
    "\n",
    "seed_dict = {'a':1,'b':2,'c':3,'d':4,'e':5}\n",
    "\n",
    "#is_random_only = True\n",
    "out_dict = {}\n",
    "M_list = [10, 15, 30, 50, 5]\n",
    "types_list  = ['active','random','randomdummy','activeopt']\n",
    "\n",
    "for type_ in types_list:\n",
    "    for M in M_list:\n",
    "        for rand_option in seed_dict.keys():\n",
    "            seed_ = seed_dict[rand_option]\n",
    "            # print(M,seed_)\n",
    "\n",
    "            uniform_samples = np.zeros(error_patch1.shape[0],dtype=np.int16)\n",
    "            nonuniform_samples = np.zeros(error_patch1.shape[0],dtype=np.int16)\n",
    "\n",
    "            np.random.seed(seed_)\n",
    "\n",
    "            if type_ == 'random':\n",
    "                selected_files = np.random.choice(tiles_avail,M, replace=False)\n",
    "                key_ = f'{M}{rand_option}r'\n",
    "\n",
    "            elif type_ == 'active':\n",
    "                selected_files = np.random.choice(tiles_avail,M, replace=False, p=weights_sample)\n",
    "                key_ = f'{M}{rand_option}'\n",
    "            elif type_ == 'randomdummy':\n",
    "                order_ = np.random.choice(keys_dummy,replace=False,size=6)\n",
    "                tiles_shuffled = create_tiles_list(order_)\n",
    "                selected_files = tiles_shuffled[:M+10]\n",
    "                key_ = f'{M}{rand_option}rd'\n",
    "\n",
    "            if type_ == 'activeopt':\n",
    "                if rand_option == 'a':\n",
    "                    # choose the top weights_ only for key a\n",
    "                    selected_files = tiles_avail[np.argsort(weights_sample)[::-1]][:M]\n",
    "                    key_ = f'{M}opt'\n",
    "                    out_dict[key_] = [x.split('manual_annotations')[-1] for x in selected_files]\n",
    "            else:\n",
    "                out_dict[key_] = [x.split('manual_annotations')[-1] for x in selected_files]\n",
    "\n",
    "            # print(f'Available {len(tiles_avail)}, selected {len(selected_files)}, Uncertainty {uniform_samples.sum()}, Quantization {nonuniform_samples.sum()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(out_dict['30a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_30a = [\n",
    "        \"/T48MWB/palm_group1_Bischel\",\n",
    "        \"/T48MUA/palm_group1_Bischel\",\n",
    "        \"/T47NND/palm_group3_Brunner\",\n",
    "        \"/T47NMD/palm_group1_Bischel\",\n",
    "        \"/T48MVB/palm_group1_Bischel\",\n",
    "        \"/T48MUB/palm_group1_Brunner\",\n",
    "        \"/T48MUB/palm_group3_Brunner\",\n",
    "        \"/T47NMD/palm_group2_Bischel\",\n",
    "        \"/T48MVB/palm_group3_Bischel\",\n",
    "        \"/T47NQD/palm_group1\",\n",
    "        \"/T46NHL/palm_group2_Stephane\",\n",
    "        \"/T48NTK/palm_group2\",\n",
    "        \"/T47MPV/palm_group1_Bischel\",\n",
    "        \"/T48MVV/palm_group3_matthew\",\n",
    "        \"/T47NNC/palm_group1_Brunner\",\n",
    "        \"/T47NPC/palm_group2_jacopo\",\n",
    "        \"/T47NLB/palm_group1_florian\",\n",
    "        \"/T47NKF/palm_group2_bischel\",\n",
    "        \"/T47NQE/palm_group1\",\n",
    "        \"/T47NRF/palm_group1\",\n",
    "        \"/T47NRA/palm_group2_Bischel\",\n",
    "        \"/T47MQV/palm_group2\",\n",
    "        \"/T47NLD/palm_group2_florian\",\n",
    "        \"/T47NPA/palm_group1_Bischel\",\n",
    "        \"/T47NQB/palm_group2\",\n",
    "        \"/T47MRU/palm_group3_Bischel\",\n",
    "        \"/T47MMU/palm_group3_julia\",\n",
    "        \"/T48NUG/palm_group1\",\n",
    "        \"/T48MUB/palm_group2_Brunner\",\n",
    "        \"/T47NPC/palm_group3_jacopo\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_30a_pf = [\n",
    "                \"/T48MWB/palm_group1_Bischel\",\n",
    "        \"/T48MUA/palm_group1_Bischel\",\n",
    "        \"/T47NND/palm_group3_Brunner\",\n",
    "        \"/T47NMD/palm_group1_Bischel\",\n",
    "        \"/T48MVB/palm_group1_Bischel\",\n",
    "        \"/T48MUB/palm_group1_Brunner\",\n",
    "        \"/T48MUB/palm_group3_Brunner\",\n",
    "        \"/T47NMD/palm_group2_Bischel\",\n",
    "        \"/T48MVB/palm_group3_Bischel\",\n",
    "        \"/T47NQD/palm_group1\",\n",
    "        \"/T46NHL/palm_group2_Stephane\",\n",
    "        \"/T48NTK/palm_group2\",\n",
    "        \"/T47MPV/palm_group1_Bischel\",\n",
    "        \"/T48MVV/palm_group3_matthew\",\n",
    "        \"/T47NNC/palm_group1_Brunner\",\n",
    "        \"/T47NPC/palm_group2_jacopo\",\n",
    "        \"/T47NLB/palm_group1_florian\",\n",
    "        \"/T47NKF/palm_group2_bischel\",\n",
    "        \"/T47NQE/palm_group1\",\n",
    "        \"/T47NRF/palm_group1\",\n",
    "        \"/T47NRA/palm_group2_Bischel\",\n",
    "        \"/T47MQV/palm_group2\",\n",
    "        \"/T47NLD/palm_group2_florian\",\n",
    "        \"/T47NPA/palm_group1_Bischel\",\n",
    "        \"/T47NQB/palm_group2\",\n",
    "        \"/T47MRU/palm_group3_Bischel\",\n",
    "        \"/T47MMU/palm_group3_julia\",\n",
    "        \"/T48NUG/palm_group1\",\n",
    "        \"/T48MUB/palm_group2_Brunner\",\n",
    "        \"/T47NPC/palm_group3_jacopo\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(set_30a_pf) == set(out_dict['30a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict['10opt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict['15br']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict['15ar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(out_dict['15ard']))\n",
    "out_dict['15ard']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(out_dict['15ard']))\n",
    "out_dict['15ard']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "save_path = '/home/pf/pfstaff/projects/andresro/barry_palm/data/2A/datasets'\n",
    "filename = save_path+'/palm4748a_activesamples_aug29.json'\n",
    "#with open(filename, 'w') as fp:\n",
    "#    json.dump(out_dict, fp)\n",
    "\n",
    "with open(filename, 'r') as fp:\n",
    "   out_dict = json.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = geopandas.GeoDataFrame(\n",
    "    geometry=geopandas.points_from_xy(rois_middle[:,0], rois_middle[:,1]))\n",
    "\n",
    "gdf_train = geopandas.GeoDataFrame(\n",
    "    geometry=geopandas.points_from_xy(rois_middle[index_train][:,0], rois_middle[index_train][:,1]))\n",
    "\n",
    "gdf_val = geopandas.GeoDataFrame(\n",
    "    geometry=geopandas.points_from_xy(rois_middle[index_val][:,0], rois_middle[index_val][:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict['50opt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))\n",
    "countries = geopandas.read_file(\"/scratch/andresro/leon_work/barry_palm/data/labels/countries/3countries.shp\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 10))\n",
    "\n",
    "## Active\n",
    "ax = plt.subplot(1,3, 1, projection=ccrs.PlateCarree())\n",
    "\n",
    "index_bool = [x.split('manual_annotations')[-1] in out_dict['15opt'] for x in tiles_gt_all]\n",
    "\n",
    "#plt.figure(figsize=(6,6))\n",
    "# ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "world.plot(ax=ax,\n",
    "    color='lightgray', edgecolor='white')\n",
    "\n",
    "countries.plot(ax=ax,\n",
    "    color='lightgray', edgecolor='white')\n",
    "\n",
    "gdf.plot(ax=ax,legend=True,marker='.',\n",
    "         edgecolor='black',\n",
    "         facecolor='none',\n",
    "         # color='green',\n",
    "         label='Available'\n",
    "        )\n",
    "\n",
    "gdf.loc[index_train].plot(ax=ax, marker='.',\n",
    "                               color='orange',\n",
    "               label='Train'\n",
    "        )\n",
    "\n",
    "gdf.loc[index_val].plot(ax=ax, marker='.',\n",
    "color='blue',\n",
    "                label='Val'\n",
    "         )\n",
    "\n",
    "\n",
    "gdf[index_bool].plot(ax=ax, marker='.',\n",
    "                                 color='red', \n",
    "                                 label='Active'\n",
    "        )\n",
    "\n",
    "\n",
    "minx, miny, maxx, maxy = gdf.total_bounds\n",
    "margin_x = 2\n",
    "margin_y = 2\n",
    "\n",
    "ax.set_xlim(minx-margin_x, maxx+margin_x)\n",
    "ax.set_ylim(miny-margin_y, maxy+margin_y)\n",
    "\n",
    "ax.legend(loc='lower left')\n",
    "\n",
    "## Naive\n",
    "ax = plt.subplot(1,3,2, projection=ccrs.PlateCarree())\n",
    "\n",
    "\n",
    "world.plot(ax=ax,\n",
    "    color='lightgray', edgecolor='white')\n",
    "\n",
    "countries.plot(ax=ax,\n",
    "    color='lightgray', edgecolor='white')\n",
    "\n",
    "gdf.plot(ax=ax,legend=True,marker='.',\n",
    "         edgecolor='black',\n",
    "         facecolor='none',\n",
    "         # color='green',\n",
    "         label='Available'\n",
    "        )\n",
    "\n",
    "gdf.loc[index_train].plot(ax=ax, marker='.',\n",
    "                               color='orange',\n",
    "               label='Train'\n",
    "        )\n",
    "\n",
    "gdf.loc[index_val].plot(ax=ax, marker='.',\n",
    "color='blue',\n",
    "                label='Val'\n",
    "         )\n",
    "\n",
    "\n",
    "gdf[index_bool].plot(ax=ax, marker='.',\n",
    "                                 color='red', \n",
    "                                 label='NaÃ¯ve'\n",
    "        )\n",
    "\n",
    "minx, miny, maxx, maxy = gdf.total_bounds\n",
    "margin_x = 2\n",
    "margin_y = 2\n",
    "\n",
    "ax.set_xlim(minx-margin_x, maxx+margin_x)\n",
    "ax.set_ylim(miny-margin_y, maxy+margin_y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#gl = ax.gridlines(\n",
    "#   crs=ccrs.PlateCarree(),\n",
    "#    draw_labels=True) #x_inline=False, y_inline=False)\n",
    "#gl.xformatter = LONGITUDE_FORMATTER\n",
    "#gl.yformatter = LATITUDE_FORMATTER\n",
    "#gl.xlabel_style = {'rotation':'vertical'}\n",
    "\n",
    "\n",
    "# ax.legend(bbox_to_anchor=(1, 0.75), loc='upper left', ncol=1,\n",
    "#          title=\"Sample Type\")\n",
    "ax.legend(loc='lower left')\n",
    "\n",
    "# plt.title('Active Learning Selected Samples\\n\\n')\n",
    "# plt.savefig(save_path_figs + 'active_learning_samples_15br.png', bbox_inches='tight', dpi= 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ref_key = '15opt'\n",
    "#label_ = 'Active'\n",
    "\n",
    "ref_key = '15br'\n",
    "label_ = 'Manual'\n",
    "\n",
    "#ref_key = '15brd'\n",
    "#label_ = 'NaÃ¯ve'\n",
    "\n",
    "index_bool = [x.split('manual_annotations')[-1] in out_dict[ref_key] for x in tiles_gt_all]\n",
    "\n",
    "world = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))\n",
    "countries = geopandas.read_file(\"/scratch/andresro/leon_work/barry_palm/data/labels/countries/3countries.shp\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "\n",
    "world.plot(ax=ax,\n",
    "    color='lightgray', edgecolor='white')\n",
    "\n",
    "countries.plot(ax=ax,\n",
    "    color='lightgray', edgecolor='white')\n",
    "\n",
    "\n",
    "gdf.plot(ax=ax,legend=True,marker='.',\n",
    "         edgecolor='black',\n",
    "         facecolor='none',\n",
    "         # color='green',\n",
    "         label='Available'\n",
    "        )\n",
    "\n",
    "\n",
    "gdf.loc[index_train].plot(ax=ax, marker='.',\n",
    "                               color='orange',\n",
    "               label='Train'\n",
    "        )\n",
    "\n",
    "gdf.loc[index_val].plot(ax=ax, marker='.',\n",
    "color='blue',\n",
    "                label='Val'\n",
    "         )\n",
    "\n",
    "gdf[index_bool].plot(ax=ax, marker='.',\n",
    "                                 color='red', \n",
    "                                 label=label_\n",
    "        )\n",
    "\n",
    "\n",
    "minx, miny, maxx, maxy = gdf.total_bounds\n",
    "margin_x = 2\n",
    "margin_y = 2\n",
    "\n",
    "ax.set_xlim(minx-margin_x, maxx+margin_x)\n",
    "ax.set_ylim(miny-margin_y, maxy+margin_y)\n",
    "\n",
    "\n",
    "\n",
    "gl = ax.gridlines(\n",
    "crs=ccrs.PlateCarree(),\n",
    "    draw_labels=False)\n",
    "#gl = ax.gridlines(\n",
    "#   crs=ccrs.PlateCarree(),\n",
    "#    draw_labels=True) #x_inline=False, y_inline=False)\n",
    "#gl.xformatter = LONGITUDE_FORMATTER\n",
    "#gl.yformatter = LATITUDE_FORMATTER\n",
    "gl.xlabel_style = {'rotation':'vertical'}\n",
    "\n",
    "\n",
    "# ax.legend(bbox_to_anchor=(1, 0.75), loc='upper left', ncol=1,\n",
    "#          title=\"Sample Type\")\n",
    "ax.legend(loc='lower left')\n",
    "\n",
    "# plt.title('Active Learning Selected Samples\\n\\n')\n",
    "plt.savefig(f'{save_path_figs}active_learning_samples_{ref_key}.pdf', bbox_inches='tight', dpi= 300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing each patch available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_average = x_sum /  n[...,np.newaxis]\n",
    "\n",
    "x_average1 = np.array([x_average[id_] for id_ in patch_id_gt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_average1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x_average.reshape(-1,320)\n",
    "index_clean = ~np.any(np.isnan(X),axis=-1)\n",
    "X = X[index_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TSNE(n_components=2).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedded = model.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_embedded[:, 0], X_embedded[:, 1] ) #, c=X_embedded[:, 2] , cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_patch1 = np.array([error_patch[id_] for id_ in patch_id_gt])\n",
    "std_patch1 = np.array([std_patches[id_] for id_ in patch_id_gt])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Palm4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_gt = '/scratch/andresro/leon_work/barry_palm/data/labels/manual_annotations'\n",
    "path_gt = '/home/pf/pfstaff/projects/andresro/barry_palm/data/labels/manual_annotations'\n",
    "\n",
    "tiles_gt_all = glob.glob(path_gt+'/T*/palm*')\n",
    "\n",
    "tiles_gt_all = sorted(tiles_gt_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rois_gt_all = np.array([gp.get_positive_area_folder(tile_) for tile_ in tiles_gt_all])\n",
    "\n",
    "def middle_roi(roi_list):\n",
    "    return np.array([roi_list[:,0]+roi_list[:,2],roi_list[:,1]+roi_list[:,3]]).T/2\n",
    "\n",
    "rois_middle = middle_roi(rois_gt_all)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = geopandas.GeoDataFrame(\n",
    "    geometry=geopandas.points_from_xy(rois_middle[:,0], rois_middle[:,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_ = '/scratch/andresro/leon_work/barry_palm/data/labels/manual_annotations'\n",
    "# index_  = [tiles_available.index(path_+x) for x in out_dict['10a']]\n",
    "# index_bool = [x in index_ for x in list(gdf.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gdf['Sample'] = 'No'\n",
    "#gdf['Sample'].loc[nonuniform_samples > 0] = 'Non Uniform'\n",
    "#gdf['Sample'].loc[uniform_samples > 0] = 'Random'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))\n",
    "countries = geopandas.read_file(\"/home/pf/pfstaff/projects/andresro/barry_palm/data/labels/countries/3countries.shp\")\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "#fig, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "#world.plot(ax=ax,\n",
    "#    color='lightgray', edgecolor='white')\n",
    "\n",
    "countries[countries.NAME_0 != 'Philippines'].plot(ax=ax,\n",
    "     color='lightgray', edgecolor='white')\n",
    "\n",
    "\n",
    "gdf.plot(ax=ax, marker='.')\n",
    "\n",
    "minx, miny, maxx, maxy = countries[countries.NAME_0 != 'Philippines'].total_bounds\n",
    "margin_x = 2\n",
    "margin_y = 2\n",
    "\n",
    "\n",
    "ax.set_xlim(minx-margin_x, maxx+margin_x)\n",
    "ax.set_ylim(miny-margin_y, maxy+margin_y)\n",
    "\n",
    "gl = ax.gridlines(\n",
    "   crs=ccrs.PlateCarree(),\n",
    "    draw_labels=True) #x_inline=False, y_inline=False)\n",
    "gl.xformatter = LONGITUDE_FORMATTER\n",
    "gl.yformatter = LATITUDE_FORMATTER\n",
    "\n",
    "gl.xlabel_style = {'rotation':'vertical'}\n",
    "\n",
    "# plt.title('Available samples')\n",
    "# plt.savefig('active_learning_samples_10a.png')\n",
    "\n",
    "#plt.savefig(save_path_figs+'3countries.png', \n",
    "##             bbox_extra_artists=(lgd,),\n",
    "#            bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing Base train and val samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isin(string_,list_):\n",
    "    return np.any([x in string_ for x in list_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(N)\n",
    "# np.random.seed(1)\n",
    "# index = np.arange(N)\n",
    "# np.random.shuffle(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valtiles_manual = 'T51NUB,T51NXB,T51NXA,T51MTT,T52MGE,T54MVS,T52MGD,T51MVU,T51MVR,T51MUR,T50MRD,T50MQD,T50MQC,T53MPT,T51MVS,T54MUC,T53MPR,T53MLV,T52MHD,T51NYB'\n",
    "valtiles_manual = valtiles_manual.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "index_val_east = np.array([i for i,x in enumerate(tiles_gt_all) if isin(x, valtiles_manual)])\n",
    "\n",
    "index_val_group2 = np.array([i for i,x in enumerate(tiles_gt_all) if 'group2' in x and not isin(x, valtiles_manual)])\n",
    "\n",
    "np.random.shuffle(index_val_group2)\n",
    "\n",
    "index_val_group2 = index_val_group2[:int(len(index_val_group2)*0.6)]\n",
    "\n",
    "index_val_pre = np.concatenate((index_val_east,index_val_group2))\n",
    "\n",
    "\n",
    "N = len(tiles_gt_all)\n",
    "\n",
    "index1 = np.array([x for x in range(N) if not x in index_val_pre])\n",
    "\n",
    "np.random.shuffle(index1)\n",
    "\n",
    "\n",
    "index_train = index1 # [:-5]\n",
    "#index_val = np.concatenate((index1[-5:],index_val_pre))\n",
    "index_val = index_val_pre\n",
    "\n",
    "print(f' total {N}, train {index_train.shape}, val {index_val.shape} (pre-selected {index_val_pre.shape})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_train = np.array(tiles_gt_all)[index_train]\n",
    "tiles_val = np.array(tiles_gt_all)[index_val]\n",
    "\n",
    "assert len(set(tiles_train).intersection(set(tiles_val))) == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tiles_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))\n",
    "countries = geopandas.read_file(\"/home/pf/pfstaff/projects/andresro/barry_palm/data/labels/countries/3countries.shp\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "world.plot(ax=ax,\n",
    "    color='lightgray', edgecolor='white')\n",
    "\n",
    "countries.plot(ax=ax,\n",
    "    color='lightgray', edgecolor='white')\n",
    "\n",
    "\n",
    "#gdf.plot(ax=ax,legend=True, marker='.',\n",
    "#          edgecolor='black',\n",
    "#          facecolor='none',\n",
    "         # color='green',\n",
    "#         label='Available - Not Selected'\n",
    "#        )\n",
    "\n",
    "\n",
    "gdf.loc[index_train].plot(ax=ax, marker='.',\n",
    "                                 color='orange',\n",
    "               label='Train'\n",
    "        )\n",
    "\n",
    "\n",
    "gdf.loc[index_val].plot(ax=ax, marker='.',\n",
    "                                color='blue',\n",
    "                label='Val'\n",
    "         )\n",
    "\n",
    "\n",
    "minx, miny, maxx, maxy = gdf.total_bounds\n",
    "margin_x = 2\n",
    "margin_y = 2\n",
    "\n",
    "ax.set_xlim(minx-margin_x, maxx+margin_x)\n",
    "ax.set_ylim(miny-margin_y, maxy+margin_y)\n",
    "\n",
    "\n",
    "gl = ax.gridlines(\n",
    "   crs=ccrs.PlateCarree(),\n",
    "    draw_labels=True) #x_inline=False, y_inline=False)\n",
    "gl.xformatter = LONGITUDE_FORMATTER\n",
    "gl.yformatter = LATITUDE_FORMATTER\n",
    "gl.xlabel_style = {'rotation':'vertical'}\n",
    "\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "#ax.legend(bbox_to_anchor=(1, 0.75), loc='upper left', ncol=1,\n",
    "#         title=\"Sample Type\")\n",
    "# plt.title('Initial dataset\\n\\n')\n",
    "# plt.savefig('active_learning_samples_10a.png')\n",
    "# plt.savefig(save_path_figs+'2_countries_base_dataset.png', bbox_inches='tight',dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[x.split('manual_annotations')[-1] for x in tiles_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict = {'train':\n",
    "           [x.split('manual_annotations')[-1] for x in tiles_train],\n",
    "           'val':\n",
    "           [x.split('manual_annotations')[-1] for x in tiles_val]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "save_path = '/home/pf/pfstaff/projects/andresro/barry_palm/data/2A/datasets'\n",
    "filename = save_path+'/palm4_base.json'\n",
    "#with open(filename, 'w') as fp:\n",
    "#    json.dump(out_dict, fp)\n",
    "with open(filename, 'rb') as fp:\n",
    "    out_dict = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading ensembe predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/scratch/andresro/leon_work/sparse/inference/palm4_simpleA9_soft_ens5'\n",
    "\n",
    "#path = '/scratch/andresro/leon_work/sparse/inference/palm4650_simpleA9_soft_mc5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tilenames = list({os.path.basename(x).split('_')[1] for x in glob.glob(path+'/R*')})\n",
    "len(tilenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every sample $i$ and dimension $j$:\n",
    "\n",
    "$$ \\sum_i ||X_i - Y ||^2_2 = \\sum_i \\big( \\sum_j X_{i,j}^2 - 2 \\sum_j (X_{i,j}*Y_j) +  \\sum_j Y_j^2 \\big)  $$\n",
    "\n",
    "$$ \\sum_j \\big( \\sum_i X_{i}^2 - 2 Y_j\\sum_i X_{i} + I* Y_j^2 \\big)  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate same locations\n",
    "for tilename in tilenames:\n",
    "    fname = f'{path}/{tilename}-5_preds_last.npz'\n",
    "    if not os.path.isfile(fname):\n",
    "        pred_dirs = glob.glob(f'{path}/*{tilename}/*.npz')\n",
    "        if len(pred_dirs) > 0:\n",
    "\n",
    "            # sum over several days in same locations\n",
    "            x_sum1 = []\n",
    "            x2_sum1 = []\n",
    "            n1 = []\n",
    "\n",
    "            for dir_ in pred_dirs:\n",
    "                preds_last = np.load(dir_)\n",
    "\n",
    "                x_sum2, x2_sum2, n2 = preds_last['x_sum'], preds_last['x2_sum'], preds_last['n']\n",
    "                x_sum1.append(x_sum2)\n",
    "                x2_sum1.append(x2_sum2)\n",
    "                n1.append(n2)\n",
    "\n",
    "            x_sum1 = np.stack(x_sum1,axis=0).sum(axis=0)\n",
    "            x2_sum1 = np.stack(x2_sum1,axis=0).sum(axis=0)\n",
    "            n1 = np.stack(n1,axis=0).sum(axis=0)\n",
    "\n",
    "            np.savez(fname,x_sum=x_sum1,x2_sum=x2_sum1,\n",
    "                            n=n1, lonlat=preds_last['lonlat'])\n",
    "            print(fname,'saved')\n",
    "        else:\n",
    "            print(f'{path}/*{tilename}/*.npz has no files')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_ = '12'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sum = []\n",
    "x2_sum = []\n",
    "n = []\n",
    "lonlat = []\n",
    "std_patches=[]\n",
    "\n",
    "is_reload = False\n",
    "fname_all = f'{path}/all_statistics.npz'\n",
    "\n",
    "if not os.path.isfile(fname_all) or is_reload:\n",
    "    print('re-processing and saving')\n",
    "    \n",
    "    for tilename in tqdm(tilenames):\n",
    "\n",
    "        fname = f'{path}/{tilename}-5_preds_last.npz'\n",
    "        if os.path.isfile(fname):\n",
    "            preds_last = np.load(fname)\n",
    "            x_sum0, x2_sum0, n0, lonlat0 = preds_last['x_sum'], preds_last['x2_sum'], preds_last['n'], preds_last['lonlat']\n",
    "            \n",
    "\n",
    "            # Adding nan values to patch statistics\n",
    "            down100_file = f'{path}/{tilename}_5_preds_reg_{compression_}_{compression_}_down100.vrt'\n",
    "            ds = gdal.Open(down100_file)\n",
    "            down_array = ds.ReadAsArray()\n",
    "            is_nan_array = interpPatches((down_array==99), ref_shape=(100,100)).squeeze() > 0.5\n",
    "            is_nan_array = is_nan_array.reshape(-1)\n",
    "\n",
    "            x_sum0[is_nan_array] = np.nan\n",
    "            x2_sum0[is_nan_array] = np.nan\n",
    "            n0[is_nan_array] = 0\n",
    "            \n",
    "            x_sum.append(x_sum0)\n",
    "            x2_sum.append(x2_sum0)\n",
    "            n.append(n0)\n",
    "            lonlat.append(lonlat0)\n",
    "\n",
    "            # Adding mean uncertainty per patch\n",
    "            std_file = f'{path}/std/{tilename}_5_preds_reg_{compression_}_std_{compression_}.tif'\n",
    "            ds = gdal.Open(std_file)\n",
    "            std_array = ds.ReadAsArray()\n",
    "            std_array[std_array == 99] = np.nan\n",
    "            std_patch = block_reduce(std_array,(110,110),np.mean)\n",
    "            std_patches.append(std_patch.reshape(-1))\n",
    "\n",
    "    x_sum = np.stack(x_sum,axis=0)\n",
    "    x2_sum = np.stack(x2_sum,axis=0)\n",
    "    n = np.stack(n,axis=0)\n",
    "    lonlat = np.stack(lonlat,axis=0)\n",
    "    std_patches = np.stack(std_patches,axis=0)\n",
    "    \n",
    "    np.savez(fname_all,x_sum=x_sum,\n",
    "            x2_sum=x2_sum,\n",
    "            n=n, lonlat=lonlat,\n",
    "            std_patches = std_patches)    \n",
    "else:\n",
    "    print('reading',fname_all)\n",
    "    preds_all = np.load(fname_all)\n",
    "\n",
    "    x_sum = preds_all['x_sum']\n",
    "    x2_sum = preds_all['x2_sum']\n",
    "    n = preds_all['n']\n",
    "    lonlat = preds_all['lonlat']\n",
    "    std_patches = preds_all['std_patches']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#is_nan_patch = np.isnan(x_sum[...,0])\n",
    "# is_nan_patch = n == 0\n",
    "is_nan_patch = np.logical_or(n == 0,np.isnan(std_patches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(n == 0).mean(), is_nan_patch.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_nan_patch.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_nan_patch.shape,std_patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(lonlat[is_nan_patch][...,0], lonlat[is_nan_patch][...,1],\n",
    "            #c=np.log(weight_sorted[:top_k]),\n",
    "            # c=weight_sorted[:top_k],\n",
    "            cmap='magma_r', marker='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(lonlat[~is_nan_patch][...,0], lonlat[~is_nan_patch][...,1],\n",
    "            #c=np.log(weight_sorted[:top_k]),\n",
    "            # c=weight_sorted[:top_k],\n",
    "            cmap='magma_r', marker='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sum[~is_nan_patch].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_global = np.sum(x_sum[~is_nan_patch], axis=0) / n.sum()\n",
    "\n",
    "# error means quantization error not std. dev.\n",
    "error_patch = (x2_sum[~is_nan_patch] - 2*x_sum[~is_nan_patch]*mean_global + n.sum() * (mean_global**2))\n",
    "\n",
    "# error_patch = np.clip(error_patch,0,np.percentile(error_patch,99))\n",
    "\n",
    "error_global = np.sum(error_patch, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_global.shape, error_patch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean_global = np.nansum(x_sum, axis=(0,1)) / n.sum()\n",
    "\n",
    "# error means quantization error not std. dev.\n",
    "#error_patch = (x2_sum - 2*x_sum*mean_global + n.sum() * (mean_global**2))\n",
    "#error_global = np.nansum(error_patch, axis=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_min = 0\n",
    "p_max = 99\n",
    "\n",
    "uncertainty = np.nan_to_num(std_patches[~is_nan_patch][...,np.newaxis])\n",
    "\n",
    "weight_div = error_patch.sum(axis=-1).reshape(-1) / np.sum(error_patch)\n",
    "\n",
    "weight_div = np.clip(weight_div,np.percentile(weight_div,p_min),np.percentile(weight_div,p_max))\n",
    "weight_div = weight_div / weight_div.sum()\n",
    "\n",
    "weight_unc = (uncertainty).sum(axis=-1).reshape(-1) / np.sum(uncertainty)\n",
    "\n",
    "#weight_unc = np.clip(weight_unc,np.percentile(weight_unc,p_min),np.percentile(weight_unc,p_max))\n",
    "#weight_unc = weight_unc / weight_unc.sum()\n",
    "\n",
    "\n",
    "weight_score = (weight_div + weight_unc)/2.0\n",
    "weight_score = weight_score/weight_score.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_perc = 99\n",
    "\n",
    "f, ax = plt.subplots(2,2, figsize=(10,5))\n",
    "ax[0,0].plot(weight_div,'x')\n",
    "ax[0,1].plot(weight_unc,'x')\n",
    "\n",
    "_ = ax[1,0].hist(np.clip(weight_div,0,np.percentile(weight_div,max_perc)))\n",
    "_ = ax[1,1].hist(np.clip(weight_unc,0,np.percentile(weight_unc,max_perc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(weight_div,90), np.percentile(weight_div,99), np.percentile(weight_div,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index = np.random.choice(weight.shape[0],100000)\n",
    "p_min = 0\n",
    "p_max = 100\n",
    "\n",
    "index_inlier = np.logical_and.reduce(\n",
    "                      (weight_div > np.percentile(weight_div,p_min),\n",
    "                       weight_div < np.percentile(weight_div,p_max),\n",
    "                       weight_unc > np.percentile(weight_unc,p_min),\n",
    "                       weight_unc < np.percentile(weight_unc,p_max),\n",
    "                      ))\n",
    "\n",
    "\n",
    "g = sns.jointplot(x=weight_div[index_inlier], y=weight_unc[index_inlier], kind=\"hex\",\n",
    "                #  xlim=(np.percentile(weight,p_min),np.percentile(weight,p_max)),\n",
    "                # ylim=(np.percentile(weight1,p_min),np.percentile(weight1,100))) #, data=tips))\n",
    "                 )\n",
    "\n",
    "g.ax_joint.set_xlabel('Quant Error')\n",
    "g.ax_joint.set_ylabel('Uncertainty')\n",
    "#g.ax_joint.set_xlim(0,1)\n",
    "\n",
    "\n",
    "# g.ax_joint.set_xlim(np.percentile(weight_div,p_min),np.percentile(weight_div,80))\n",
    "#g.ax_joint.set_ylim(np.percentile(weight1,p_min),np.percentile(weight1,100))\n",
    "# g.ax_joint.set_xticks([])\n",
    "# g.ax_joint.set_yticks([])\n",
    "\n",
    "\n",
    "\n",
    "# plt.savefig(save_path_figs + 'uncertainty_vs_quant_all.png', bbox_inches='tight', dpi= 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_sorted = np.argsort(weight_score)[::-1] \n",
    "\n",
    "weight_sorted = weight_score[ids_sorted]\n",
    "lonlat_sorted = lonlat[~is_nan_patch][ids_sorted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_train = [x.split('manual_annotations')[-1] in out_dict['train'] for x in tiles_gt_all]\n",
    "index_val = [x.split('manual_annotations')[-1] in out_dict['val'] for x in tiles_gt_all]\n",
    "\n",
    "# index_bool = [x.split('manual_annotations')[-1] in out_dict['30opt'] for x in tiles_gt_all]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_active = 30\n",
    "\n",
    "gdf = geopandas.GeoDataFrame(\n",
    "    geometry=geopandas.points_from_xy(rois_middle[:,0], rois_middle[:,1]))\n",
    "\n",
    "gdf = geopandas.GeoDataFrame(\n",
    "    geometry=geopandas.points_from_xy(rois_middle[:,0], rois_middle[:,1]))\n",
    "\n",
    "\n",
    "gdf_active = geopandas.GeoDataFrame(\n",
    "    geometry=geopandas.points_from_xy(lonlat_sorted[:N_active,0], lonlat_sorted[:N_active,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 1000\n",
    "plt.scatter(lonlat_sorted[:top_k,0], lonlat_sorted[:top_k,1],\n",
    "            #c=np.log(weight_sorted[:top_k]),\n",
    "            c=weight_sorted[:top_k],\n",
    "            cmap='magma_r', marker='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))\n",
    "countries = geopandas.read_file(\"/scratch/andresro/leon_work/barry_palm/data/labels/countries/3countries.shp\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "\n",
    "world.plot(ax=ax,\n",
    "    color='lightgray', edgecolor='white')\n",
    "\n",
    "#countries.plot(ax=ax,\n",
    "#    color='lightgray', edgecolor='white')\n",
    "\n",
    "gdf.loc[index_train].plot(ax=ax, marker='.',\n",
    "                               color='orange',\n",
    "               label='Train'\n",
    "        )\n",
    "\n",
    "gdf.loc[index_val].plot(ax=ax, marker='.',\n",
    "color='blue',\n",
    "                label='Val'\n",
    "         )\n",
    "\n",
    "gdf_active.plot(ax=ax, marker='.',\n",
    "                                 color='red', \n",
    "                                 label='Active'\n",
    "        )\n",
    "\n",
    "\n",
    "minx, miny, maxx, maxy = gdf.total_bounds\n",
    "margin_x = 2\n",
    "margin_y = 2\n",
    "\n",
    "\n",
    "minx1, miny1, maxx1, maxy1 = gdf_active.total_bounds\n",
    "\n",
    "minx = min(minx,minx1)\n",
    "miny = min(miny,miny1)\n",
    "maxx = max(maxx,maxx1)\n",
    "maxy = max(maxy,maxy1)\n",
    "\n",
    "\n",
    "ax.set_xlim(minx-margin_x, maxx+margin_x)\n",
    "ax.set_ylim(miny-margin_y, maxy+margin_y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "gl = ax.gridlines(\n",
    "   crs=ccrs.PlateCarree(),\n",
    "    draw_labels=True) #x_inline=False, y_inline=False)\n",
    "gl.xformatter = LONGITUDE_FORMATTER\n",
    "gl.yformatter = LATITUDE_FORMATTER\n",
    "gl.xlabel_style = {'rotation':'vertical'}\n",
    "\n",
    "\n",
    "# ax.legend(bbox_to_anchor=(1, 0.75), loc='upper left', ncol=1,\n",
    "#          title=\"Sample Type\")\n",
    "#ax.legend() # loc='lower left')\n",
    "\n",
    "# plt.title('Active Learning Selected Samples\\n\\n')\n",
    "# plt.savefig(save_path_figs + 'active_learning_samples_10opt.', bbox_inches='tight', dpi= 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simplekml\n",
    "\n",
    "kml = simplekml.Kml()\n",
    "for key, val in enumerate(lonlat_sorted[:N_active]):\n",
    "    #lat, lon = gp.to_latlon(val[1],val[0],ds_pred)\n",
    "    lon, lat = val\n",
    "    pnt = kml.newpoint(name=str(key), coords=[(lon, lat)])\n",
    "    \n",
    "#Â kml.save(f'/scratch2/Dropbox/Dropbox/0_phd/temp/{N_active}_New_samples_palm4.kml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using core-set for a k-mean solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample C samples to create a core-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(n[~is_nan_patch] == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_nan_patch.shape, weight_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 4\n",
    "\n",
    "bins_long = np.linspace(np.min(lonlat[~is_nan_patch][:,0]),np.max(lonlat[~is_nan_patch][:,0]),n_bins+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_index = []\n",
    "\n",
    "for bin_ in range(n_bins):\n",
    "    bins_index.append(np.logical_and(bins_long[bin_]<=lonlat[~is_nan_patch][:,0], bins_long[bin_+1]>=lonlat[~is_nan_patch][:,0]))\n",
    "    \n",
    "bins_index = np.array(bins_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 100000\n",
    "N_samples = weight_score.shape[0]\n",
    "\n",
    "# w_ = weight_score[n[~is_nan_patch] > 0]/weight_score[n[~is_nan_patch] > 0].sum()\n",
    "# w_ = weight_score[n.reshape(-1) > 0]/weight_score[n.reshape(-1) > 0].sum()\n",
    "# w_ = weight_score[np.logical_and(n.reshape(-1) > 0,np.percentile(weight_score,99))]/weight_score[np.logical_and(n.reshape(-1) > 0,np.percentile(weight_score,99))].sum()\n",
    "# w_ = weight_unc[n.reshape(-1) > 0]/weight_unc[n.reshape(-1) > 0].sum()\n",
    "\n",
    "samples_id = np.arange(N_samples) # [n[~is_nan_patch] > 0]\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "#core_set = np.random.choice(samples_id,size=C, replace=True, p=weight_score)\n",
    "\n",
    "# ids_sorted = np.argsort(weight_score)[::-1] \n",
    "# core_set = samples_id[ids_sorted][:C]\n",
    "\n",
    "## top-k with bins\n",
    "\n",
    "N_bins = (C *bins_index.sum(axis=1) / bins_index.sum()).astype(np.int)\n",
    "\n",
    "\n",
    "core_set = []\n",
    "for bin_ in range(n_bins):\n",
    "    index_ = bins_index[bin_]\n",
    "    ids_sorted = np.argsort(weight_score[index_])[::-1]\n",
    "    samples_id_sorted = samples_id[index_][ids_sorted]\n",
    "    n_ = N_bins[bin_]        \n",
    "    core_set.append(samples_id_sorted[:C])\n",
    "\n",
    "core_set = np.stack(core_set)\n",
    "\n",
    "## top-k no bins\n",
    "# ids_sorted = np.argsort(weight_score)[::-1] \n",
    "# core_set = samples_id[ids_sorted][:C]\n",
    "\n",
    "# ids_sorted = np.argsort(weight_unc)[::-1] \n",
    "# core_set = samples_id[ids_sorted][:C//2]\n",
    "# ids_sorted = np.argsort(weight_div)[::-1] \n",
    "# core_set = np.concatenate((core_set,samples_id[ids_sorted][:C//2]))\n",
    "\n",
    "core_set_weight = 1 / (C* weight_score[core_set.reshape(-1)])\n",
    "# core_set_weight = 1 / (C * weight_unc[core_set])\n",
    "\n",
    "# key_ = f'{M}{rand_option}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(lonlat[~is_nan_patch][:,0], lonlat[~is_nan_patch][:,1],\n",
    "            #c=np.log(weight_sorted[:top_k]),\n",
    "            c=weight_score,\n",
    "            cmap='magma_r', marker='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_ = 1\n",
    "\n",
    "plt.scatter(lonlat[~is_nan_patch][bins_index[i_]][:,0],\n",
    "            lonlat[~is_nan_patch][bins_index[i_]][:,1],\n",
    "            #c=np.log(weight_sorted[:top_k]),\n",
    "            c=weight_score[bins_index[i_]],\n",
    "            cmap='magma_r', marker='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_ = 3\n",
    "\n",
    "plt.scatter(lonlat[~is_nan_patch][core_set[i_]][:,0],\n",
    "            lonlat[~is_nan_patch][core_set[i_]][:,1],\n",
    "            #c=np.log(weight_sorted[:top_k]),\n",
    "            c=weight_score[core_set[i_]],\n",
    "            cmap='magma_r', marker='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.jointplot(x=weight_div[core_set], y=weight_unc[core_set], kind=\"hex\")\n",
    "\n",
    "g.ax_joint.set_xlabel('Quant Error')\n",
    "g.ax_joint.set_ylabel('Uncertainty')\n",
    "# g.ax_joint.set_xlim(np.percentile(weight_div,p_min),np.percentile(weight_div,80))\n",
    "#g.ax_joint.set_ylim(np.percentile(weight1,p_min),np.percentile(weight1,100))\n",
    "# g.ax_joint.set_xticks([])\n",
    "# g.ax_joint.set_yticks([])\n",
    "\n",
    "\n",
    "# plt.savefig(save_path_figs + 'uncertainty_vs_quant_all.png', bbox_inches='tight', dpi= 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreset_lonlat_coords = lonlat[~is_nan_patch][core_set]\n",
    "\n",
    "x_mean_coreset = x_sum[~is_nan_patch][core_set] / (n[~is_nan_patch][core_set][...,np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean_coreset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 50\n",
    "\n",
    "N_bins = (n_clusters *bins_index.sum(axis=1) / bins_index.sum()).astype(np.int)\n",
    "center_pixels_index = []\n",
    "for i in range(n_bins):\n",
    "    model = KMeans(n_clusters=N_bins[i], random_state=0)\n",
    "#     model.fit(x_mean_coreset[i*C:(i+1)*C], sample_weight=core_set_weight[i*C:(i+1)*C])\n",
    "    model.fit(x_mean_coreset[i], sample_weight=core_set_weight[i])\n",
    "\n",
    "    index_center = np.argmin(model.transform(x_mean_coreset[i]), axis=0) # + i*C\n",
    "    \n",
    "    center_pixels_index.append(index_center)\n",
    "\n",
    "# center_pixels_index = np.concatenate(center_pixels_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers_index = []\n",
    "for i in range(n_bins):\n",
    "    centers_index.append(center_pixels_index[i]+i*C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core_set = core_set.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_clusters = 50\n",
    "\n",
    "# model = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "# model.fit(x_mean_coreset, sample_weight=core_set_weight)\n",
    "\n",
    "# cluster_id = model.predict(x_mean_coreset)\n",
    "\n",
    "# center_pixels_index = np.argmin(model.transform(x_mean_coreset), axis=0)\n",
    "# center_lonlat_coords = coreset_lonlat_coords[center_pixels_index]\n",
    "\n",
    "# # center_lonlat_coords = center_lonlat_coords[np.argsort(center_lonlat_coords[:,0],axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.jointplot(x=weight_div[core_set], y=weight_unc[core_set], kind=\"hex\")\n",
    "\n",
    "\n",
    "g.ax_joint.set_xlabel('Quant Error')\n",
    "g.ax_joint.set_ylabel('Uncertainty')\n",
    "# g.ax_joint.set_xlim(np.percentile(weight_div,p_min),np.percentile(weight_div,80))\n",
    "#g.ax_joint.set_ylim(np.percentile(weight1,p_min),np.percentile(weight1,100))\n",
    "# g.ax_joint.set_xticks([])\n",
    "# g.ax_joint.set_yticks([])\n",
    "\n",
    "for i in range(n_bins):\n",
    "    g.ax_joint.scatter(x=weight_div[core_set[i]][center_pixels_index[i]],\n",
    "                    y=weight_unc[core_set[i]][center_pixels_index[i]], c='red')\n",
    "\n",
    "\n",
    "plt.savefig(save_path_figs + 'uncertainty_vs_quant_coreset_sept30.png', bbox_inches='tight', dpi= 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "Y = TSNE(n_components=2, n_jobs=-1).fit_transform(x_mean_coreset.reshape(-1,320))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.jointplot(Y[...,0],Y[...,1],kind='kde')\n",
    "\n",
    "# sns.jointplot(Y[patch_id_gt][:,0],Y[patch_id_gt][:,1])\n",
    "Y_ = Y.reshape(n_bins,-1,2)\n",
    "for i in range(n_bins):\n",
    "    g.ax_joint.scatter(x=Y_[i][center_pixels_index[i]][...,0],\n",
    "                       y=Y_[i][center_pixels_index[i]][...,1],\n",
    "                       c='red')\n",
    "\n",
    "# g.ax_joint.scatter(Y[center_pixels_index][:,0],Y[center_pixels_index][:,1], c='red')\n",
    "# g.ax_joint.scatter(Y[patch_id_gt][center_pixels_index][:,0],Y[patch_id_gt][center_pixels_index][:,1], c='white', marker='.', facecolor=None)\n",
    "\n",
    "\n",
    "plt.savefig(save_path_figs + 'tsne_palm4_ALset.pdf', bbox_inches='tight', dpi= 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_lonlat_coords = []\n",
    "for i in range(n_bins):\n",
    "    center_lonlat_coords.append(coreset_lonlat_coords[i][center_pixels_index[i]])\n",
    "center_lonlat_coords = np.concatenate(center_lonlat_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_coreset = geopandas.GeoDataFrame(\n",
    "    geometry=geopandas.points_from_xy(coreset_lonlat_coords.reshape(-1,2)[:,0], coreset_lonlat_coords.reshape(-1,2)[:,1]))\n",
    "\n",
    "gdf_active = geopandas.GeoDataFrame(\n",
    "    geometry=geopandas.points_from_xy(center_lonlat_coords.reshape(-1,2)[:,0], center_lonlat_coords.reshape(-1,2)[:,1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))\n",
    "#countries = geopandas.read_file(\"/scratch/andresro/leon_work/barry_palm/data/labels/countries/3countries.shp\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "\n",
    "# gdf_coreset.plot(ax=ax, marker='o',\n",
    "#                                  color='lightgray', \n",
    "#                                  label='Coreset'\n",
    "#         )\n",
    "\n",
    "\n",
    "world.plot(ax=ax,\n",
    "    color='lightgray', edgecolor='white')\n",
    "\n",
    "#countries.plot(ax=ax,\n",
    "#    color='lightgray', edgecolor='white')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "gdf.loc[index_train].plot(ax=ax, marker='.',\n",
    "                               color='orange',\n",
    "               label='Train'\n",
    "        )\n",
    "\n",
    "gdf.loc[index_val].plot(ax=ax, marker='.',\n",
    "color='blue',\n",
    "                label='Val'\n",
    "         )\n",
    "\n",
    "gdf_active.plot(ax=ax, marker='.',\n",
    "                                 color='red', \n",
    "                                 label='Active'\n",
    "        )\n",
    "\n",
    "\n",
    "minx, miny, maxx, maxy = gdf.total_bounds\n",
    "margin_x = 2\n",
    "margin_y = 2\n",
    "\n",
    "\n",
    "minx1, miny1, maxx1, maxy1 = gdf_coreset.total_bounds\n",
    "\n",
    "minx = min(minx,minx1)\n",
    "miny = min(miny,miny1)\n",
    "maxx = max(maxx,maxx1)\n",
    "maxy = max(maxy,maxy1)\n",
    "\n",
    "\n",
    "ax.set_xlim(minx-margin_x, maxx+margin_x)\n",
    "ax.set_ylim(miny-margin_y, maxy+margin_y)\n",
    "\n",
    "\n",
    "gl = ax.gridlines(\n",
    "   crs=ccrs.PlateCarree(),\n",
    "    draw_labels=True) #x_inline=False, y_inline=False)\n",
    "gl.xformatter = LONGITUDE_FORMATTER\n",
    "gl.yformatter = LATITUDE_FORMATTER\n",
    "gl.xlabel_style = {'rotation':'vertical'}\n",
    "\n",
    "\n",
    "# ax.legend(bbox_to_anchor=(1, 0.75), loc='upper left', ncol=1,\n",
    "#          title=\"Sample Type\")\n",
    "ax.legend() # loc='lower left')\n",
    "\n",
    "# plt.title('Active Learning Selected Samples\\n\\n')\n",
    "# \n",
    "\n",
    "plt.savefig(save_path_figs + 'palm4_30_ALsamples_sept25.pdf', bbox_inches='tight', dpi= 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simplekml\n",
    "\n",
    "kml = simplekml.Kml()\n",
    "for key, val in enumerate(center_lonlat_coords):\n",
    "    #lat, lon = gp.to_latlon(val[1],val[0],ds_pred)\n",
    "    lon, lat = val\n",
    "    pnt = kml.newpoint(name=str(key), coords=[(lon, lat)])\n",
    "    \n",
    "kml.save(f'/scratch2/Dropbox/Dropbox/0_phd/temp/{n_clusters}samples_coreset100k_combinedscore.kml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#path = os.path.dirname(data_dir)\n",
    "pred_tifs = glob.glob(f'{path}/{tilename}*.tif')\n",
    "std_tifs = glob.glob(f'{path}/std/{tilename}*std*.tif')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = gdal.Open(std_tifs[0])\n",
    "\n",
    "patch_size = 150\n",
    "border = 20\n",
    "patch1 = 150  - border*2\n",
    "print(patch1)\n",
    "ds.RasterXSize/patch1, ds.RasterYSize /patch1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = gdal.Open(std_tifs[0])\n",
    "preds_array = ds.ReadAsArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_array = ds.ReadAsArray()\n",
    "\n",
    "preds_array[preds_array == 99] = np.nan\n",
    "preds_array1 = block_reduce(preds_array,(110,110),np.mean)\n",
    "plt.imshow(preds_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "preds_array1 = block_reduce(preds_array,(110,110),np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(preds_array1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = preds_last['lonlat'][:,0].reshape(100,100)\n",
    "y = preds_last['lonlat'][:,1].reshape(100,100)\n",
    "z = error_patch.mean(axis=1).reshape(100,100)\n",
    "#X, Y = np.meshgrid(x, y)\n",
    "\n",
    "plt.pcolormesh(x, y,z) # shading='auto') #, cmap='Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = gdal.Open(path_)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_path = '/scratch/andresro/leon_work/sparse/inference/'\n",
    "base_path = '/home/pf/pfstaff/projects/andresro/sparse/inference_leon/'\n",
    "path_ = base_path + \"borneo_simpleA9_mc10/0_EPSG_32749_down400.tif\"\n",
    "ds_pred = gdal.Open(path_)\n",
    "array_preds = ds_pred.ReadAsArray()\n",
    "ref_proj = ds_pred.GetGeoTransform()\n",
    "\n",
    "path_ = base_path +\"borneo_simpleA9_mc10/std/0_EPSG_32749_down400.tif\"\n",
    "ds = gdal.Open(path_)\n",
    "warp_opts = gdal.WarpOptions(\n",
    "    format=\"VRT\", \n",
    "    xRes=ref_proj[1],\n",
    "    yRes=ref_proj[5],\n",
    "    dstNodata='nan')\n",
    "gdal.Warp('/vsimem/std_reprojected.vrt', ds, options=warp_opts)\n",
    "ds = gdal.Open('/vsimem/std_reprojected.vrt')\n",
    "array_std = ds.ReadAsArray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title('Predicted density')\n",
    "plt.imshow(np.clip(array_preds,0.4,1))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Model Uncertainty')\n",
    "plt.imshow(np.clip(array_std,0.3,0.8), cmap='Reds')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.savefig('/scratch2/Dropbox/Dropbox/0_phd/temp/density_uncertainty_borneo.png',bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.subplot(121)\n",
    "\n",
    "plt.title('Model Uncertainty')\n",
    "plt.imshow(np.clip(array_std,0.3,0.8), cmap='Reds')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(122)\n",
    "\n",
    "array1 = np.logical_and(array_std >0.4, array_preds > 0.15)*1.0\n",
    "array1[np.isnan(array_std)] = np.nan\n",
    "plt.title('High uncertainty and medium or higher density')\n",
    "plt.imshow(array1, cmap = 'magma')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.savefig('/scratch2/Dropbox/Dropbox/0_phd/temp/density_uncertainty_borneo_1.png',bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_coords = np.argwhere(np.logical_and(array_std > 0.4, array_preds > 0.15))\n",
    "latlon_coords = np.array([gp.to_latlon(y,x,ds_pred) for x, y in pixel_coords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 15\n",
    "\n",
    "model = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "model.fit(pixel_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id = model.predict(pixel_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_pixels_index = np.argmin(model.transform(pixel_coords), axis=0)\n",
    "\n",
    "center_latlon_coords = latlon_coords[center_pixels_index]\n",
    "\n",
    "center_latlon_coords = center_latlon_coords[np.argsort(center_latlon_coords[:,0],axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.scatter(-pixel_coords[:,0], pixel_coords[:,1], c=cluster_id, cmap='tab20',marker='.')\n",
    "#plt.scatter(-center_pixel_coords[:,0], center_pixel_coords[:,1], marker='x')\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.subplot(121)\n",
    "\n",
    "array1 = np.logical_and(array_std >0.4, array_preds > 0.15)*1.0\n",
    "array1[np.isnan(array_std)] = np.nan\n",
    "plt.title('High uncertainty and medium or higher density')\n",
    "plt.imshow(array1, cmap = 'magma')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(122)\n",
    "\n",
    "plt.title('Cluster areas')\n",
    "\n",
    "plt.scatter(latlon_coords[:,1], latlon_coords[:,0], c=cluster_id, cmap='tab20',marker='.')\n",
    "plt.scatter(center_latlon_coords[:,1], center_latlon_coords[:,0], marker='x')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.savefig('/scratch2/Dropbox/Dropbox/0_phd/temp/density_uncertainty_borneo_centers.png',bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_latlon_coords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simplekml\n",
    "\n",
    "kml = simplekml.Kml()\n",
    "for key, val in enumerate(center_latlon_coords):\n",
    "    #lat, lon = gp.to_latlon(val[1],val[0],ds_pred)\n",
    "    lat, lon = val\n",
    "    pnt = kml.newpoint(name=str(key), coords=[(lon, lat)])\n",
    "    \n",
    "#kml.save(f'/scratch2/Dropbox/Dropbox/0_phd/temp/{n_clusters}_New_samples.kml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $PF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "#no_na = np.logical_and(~np.isnan(array_preds.reshape(-1)),~np.isnan(array_std.reshape(-1)))\n",
    "df = pd.DataFrame({\"pred\": array_preds.reshape(-1), \"std\": array_std.reshape(-1)})\n",
    "df.dropna(inplace=True)\n",
    "#\n",
    "#df = df[np.logical_and(df['pred'] > 0.2,df['std'] > 0.0)]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hexplot = sns.jointplot(\"pred\", \"std\", data = df ,kind=\"hex\", space=1,bins='log') #, xlim=(0,0.5), ylim=(0,0.6))\n",
    "#hexplot.fig.axhline(y=0.5, color='r', linestyle='-')\n",
    "\n",
    "plt.subplots_adjust(left=0.2, right=0.8, top=0.8, bottom=0.2)  # shrink fig so cbar is visible\n",
    "# make new ax object for the cbar\n",
    "cbar_ax = hexplot.fig.add_axes([.85, .25, .05, .4])  # x, y, width, height\n",
    "plt.colorbar(cax=cbar_ax)\n",
    "\n",
    "hexplot.ax_joint.plot([0, 1.25], [0.4, 0.4], linewidth=2, c='r')\n",
    "hexplot.ax_joint.plot([0.15, 0.15], [0, 0.55], linewidth=2, c='g')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H.shape, X.shape,Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 100\n",
    "xedges = np.linspace(0,1.2,n_bins)\n",
    "yedges = np.linspace(0,0.5,n_bins)\n",
    "\n",
    "#xedges = np.logspace(-.0,1.2,500)\n",
    "#yedges = np.logspace(0,0.5,500)\n",
    "\n",
    "H, xedges, yedges = np.histogram2d(df['pred'], df['std'], bins=(xedges, yedges))\n",
    "H = H.T\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(111,\n",
    "        aspect='equal')\n",
    "X, Y = np.meshgrid(xedges, yedges)\n",
    "im = ax.pcolormesh(X, Y, np.log(H+1e-7), cmap='Blues')\n",
    "#ax.pcolormesh(X, Y, H, cmap='Blues')\n",
    "\n",
    "ax = plt.subplots_adjust(left=0.2, right=0.8, top=0.8, bottom=0.2)  # shrink fig so cbar is visible\n",
    "# make new ax object for the cbar\n",
    "plt.colorbar(im, ax)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hexplot = sns.jointplot(\"pred\", \"std\", data = df ,kind=\"hex\", space=1,\n",
    "                        bins='log',\n",
    "                        xlim=(0.15,1.25), ylim=(0.4,0.6))\n",
    "\n",
    "plt.subplots_adjust(left=0.2, right=0.8, top=0.8, bottom=0.2)  # shrink fig so cbar is visible\n",
    "# make new ax object for the cbar\n",
    "cbar_ax = hexplot.fig.add_axes([.85, .25, .05, .4])  # x, y, width, height\n",
    "plt.colorbar(cax=cbar_ax)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(\"pred\", \"std\", data = df,kind=\"hex\",ratio=3, xlim=(-.1,0.2), ylim=(0.1,0.3),bins='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(\"pred\", \"std\", data = df[np.logical_and(df['pred'] > 0.15,df['std'] > 0.4)] ,bins='log',kind=\"hex\") #, xlim=(0,0.5), ylim=(0,0.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(df[np.logical_and(df['pred'] > 0.2,df['std'] > 0.2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#X_embedded = TSNE(n_components=2).fit_transform(np.array(df[np.logical_and(df['pred'] > 0.2,df['std'] > 0.2)]))\n",
    "X_embedded = TSNE(n_components=2).fit_transform(np.array(df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_embedded[:,0], X_embedded[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_ = 'T49MET'\n",
    "path_ = f\"/scratch/andresro/leon_work/sparse/inference/borneo_simpleA9_mc10/{tile_}_10_preds_reg_0_0_down20_EPSG_32749.vrt\"\n",
    "ds = gdal.Open(path_)\n",
    "array_preds = ds.ReadAsArray()\n",
    "\n",
    "path_ = f\"/scratch/andresro/leon_work/sparse/inference/borneo_simpleA9_mc10/std/{tile_}_10_preds_reg_0_std_0_down20_EPSG_32749.vrt\"\n",
    "\n",
    "ds = gdal.Open(path_)\n",
    "array_std = ds.ReadAsArray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.clip(array_preds,0.4,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.clip(array_std,0.3,0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "#no_na = np.logical_and(~np.isnan(array_preds.reshape(-1)),~np.isnan(array_std.reshape(-1)))\n",
    "df = pd.DataFrame({\"pred\": array_preds.reshape(-1), \"std\": array_std.reshape(-1)})\n",
    "df.dropna(inplace=True)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(\"pred\", \"std\", data = df ,kind=\"kde\") , xlim=(-0.25,1.5), ylim=(0,0.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gdal233]",
   "language": "python",
   "name": "conda-env-gdal233-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
