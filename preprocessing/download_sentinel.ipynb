{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Sentinel-2 imagery\n",
    "\n",
    "This script can be used to download all sentinel-2 images inside a geographical area defined by a `.shp` file and certain time-range. For large areas, this process will take a long time since the bandwitdh for downloading from ESA is not too large, you can use this script to query only the tiles to be downloaded and save it as a `.pkl` file. Then you can use `download1C_df.sh` to load the `.pkl` file and download then.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal, osr, ogr, gdalconst\n",
    "import os\n",
    "import numpy as np\n",
    "from shapely.geometry import mapping, shape\n",
    "from shapely.wkt import loads\n",
    "from shapely.geometry import Polygon\n",
    "import json\n",
    "import xml.etree.ElementTree as ET \n",
    "import glob\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from sentinelsat import SentinelAPI, read_geojson, geojson_to_wkt\n",
    "from datetime import date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the .shp files from [here](https://www.diva-gis.org/gdata) for each country of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the country shape file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/scratch/andresro/leon_work/barry_palm/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/scratch/andresro/leon_work/barry_palm/data'\n",
    "# root_dir = '/home/pf/pfstaff/projects/andresro/barry_palm/data'\n",
    "\n",
    "# country='Malaysia'\n",
    "country = 'Indonesia'\n",
    "\n",
    "if country == 'Phillipines':\n",
    "    loc='asia_2019'\n",
    "    # loc='phillipines_2017'\n",
    "    adm1_path = '/home/pf/pfstaff/projects/andresro/data/countries/phillipines/PHL_adm1.shp'\n",
    "    NAME = 'all'\n",
    "#     NAME = ['Palawan','Cebu','Davao del Norte','Davao del Sur','Davao Oriental','Batangas','Quezon','Rizal','Laguna']\n",
    "elif country == 'Malaysia':\n",
    "    loc='asia_2019'\n",
    "#     loc='palmcountries_2017'\n",
    "    NAME = 'all'\n",
    "    adm1_path = '/home/pf/pfstaff/projects/andresro/data/countries/malaysia/MYS_adm1.shp'\n",
    "#     NAME=['Sarawak']\n",
    "elif country == 'Indonesia':\n",
    "    loc='asia_2019'\n",
    "#     loc='palmcountries_2017'\n",
    "    adm1_path = '/home/pf/pfstaff/projects/andresro/data/countries/indonesia/IDN_adm1.shp'\n",
    "    NAME = 'all'\n",
    "#     NAME = ['Kalimantan Barat','Riau','Sulawesi Barat','Sulawesi Tengah','Sulawesi Utara','Gorontalo']\n",
    "\n",
    "\n",
    "product_dir = os.path.join(root_dir,'1C',loc,'PRODUCT')\n",
    "save_dir = os.path.join(root_dir,'1C','dataframes_download')\n",
    "save_dir1 = os.path.join(save_dir,loc)\n",
    "if not os.path.exists(save_dir1):\n",
    "    os.makedirs(save_dir1)\n",
    "    \n",
    "if not os.path.exists(product_dir):\n",
    "    os.makedirs(product_dir)\n",
    "    \n",
    "print('product_dir: ', product_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fieldname = 'NAME_1'\n",
    "\n",
    "shp = ogr.Open(adm1_path)\n",
    "lyr = shp.GetLayer(0)\n",
    "lyrdf =lyr.GetLayerDefn()\n",
    "id_ = lyrdf.GetFieldIndex(fieldname)\n",
    "    \n",
    "print('Total features', lyr.GetFeatureCount())\n",
    "features_extent = {}\n",
    "features_polygones = {}\n",
    "for i in range(lyr.GetFeatureCount()):\n",
    "    feat = lyr.GetFeature(i)\n",
    "    value =feat.GetField(id_)\n",
    "#     if value == name_:\n",
    "    geom=feat.GetGeometryRef()\n",
    "    extent = geom.GetEnvelope()\n",
    "    lon1,lat1 = extent[0],extent[2]\n",
    "    lon2,lat2 = extent[1],extent[3]\n",
    "    wkt_ext = f'POLYGON(({lon1} {lat1}, {lon1} {lat2}, {lon2} {lat2},  {lon2} {lat1},  {lon1} {lat1} ))'\n",
    "    features_extent[value] = wkt_ext\n",
    "    features_polygones[value]=loads(geom.ExportToWkt())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Sentinel-2 tile names in Polygon\n",
    "\n",
    "You can download the sentinel-2 tiles from [here](https://sentinel.esa.int/web/sentinel/missions/sentinel-2/data-products) as .xml and convert them as .shp with QGIS to create ´Features.shp´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentinel2_tiles_path = '/home/pf/pfstaff/projects/nlang_HCS/data/Sentinel2_mission/sentinel2_tiles/Features.shp'\n",
    "driver = ogr.GetDriverByName('ESRI Shapefile')\n",
    "sentinel2_tiles = driver.Open(sentinel2_tiles_path, 0) # 0 means read-only. 1 means writeable.\n",
    "\n",
    "print('Opened {}'.format(sentinel2_tiles_path))\n",
    "layer = sentinel2_tiles.GetLayer()\n",
    "featureCount = layer.GetFeatureCount()\n",
    "print('Number of layers: ', sentinel2_tiles.GetLayerCount())\n",
    "# print(layer.GetLayerDefn())\n",
    "print(\"Number of features: \", featureCount)\n",
    "\n",
    "\n",
    "def getGeom(Shapefile, shapely = True):\n",
    "    feature_dict={}\n",
    "    n_layers = Shapefile.GetLayerCount()\n",
    "    wkt_list  = []\n",
    "    for _ in range(n_layers):\n",
    "        Shapefile_layer = Shapefile.GetLayer()\n",
    "\n",
    "        n_points = Shapefile_layer.GetFeatureCount()\n",
    "\n",
    "        for _ in range(n_points):\n",
    "            feat = Shapefile_layer.GetNextFeature()\n",
    "            if feat:\n",
    "                name = feat.GetFieldAsString(\"Name\")\n",
    "                geom = feat.geometry().ExportToWkt()\n",
    "                if shapely:\n",
    "                    geom = loads(geom)\n",
    "                wkt_list.append(geom)\n",
    "                # save in dictionary\n",
    "                feature_dict[name]=geom\n",
    "\n",
    "    print('{} geometries loaded'.format(len(wkt_list)))\n",
    "\n",
    "    return wkt_list, feature_dict\n",
    "    \n",
    "tiles_geometry, feature_dict = getGeom(Shapefile=sentinel2_tiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "roi_tiles_per_feature = {}\n",
    "\n",
    "for name, poly in features_polygones.items():\n",
    "\n",
    "    for tile, tile_poly in feature_dict.items():\n",
    "        if poly.intersects(tile_poly):\n",
    "            if name in roi_tiles_per_feature.keys():                \n",
    "                roi_tiles_per_feature[name].append(tile)\n",
    "            else:\n",
    "                roi_tiles_per_feature[name] = [tile]\n",
    "#     print(name,len(roi_tiles_per_feature[name]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(roi_tiles_per_feature, orient='index')\n",
    "\n",
    "df['count'] = df.shape[1]-df.isnull().sum(axis=1)\n",
    "df = df.sort_values('count',ascending=False)\n",
    "df['count'].sum()\n",
    "# for key, value in roi_tiles_per_feature.items():\n",
    "#     print(key,len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = set(np.array(df.drop('count', axis=1)).flatten())\n",
    "len(a)\n",
    "df['tiles_country'] = len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = []\n",
    "for key, val in roi_tiles_per_feature.items():\n",
    "    for tile in val:\n",
    "        df1.append([key,tile])\n",
    "df1 = pd.DataFrame(columns=['Region','Tile'], data=df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_tiles = {tile for _,val in roi_tiles_per_feature.items() for tile in val}\n",
    "\n",
    "df1 = []\n",
    "for tile in set_tiles:\n",
    "    keys = [key for key, val in roi_tiles_per_feature.items() if tile in val]\n",
    "    df1.append([', '.join(keys),tile])\n",
    "df1 = pd.DataFrame(columns=['Region','Tile'], data=df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = save_dir1+'/'+f'Tiles_per_region_{country}_1.csv'\n",
    "# df1.to_csv(filename)\n",
    "# print(filename, 'saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tiles = df\n",
    "df_tiles.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query tiles from SCIHUB server\n",
    "\n",
    "If you do not have a scihub account create one [here](https://scihub.copernicus.eu/dhus/#/self-registration).\n",
    "\n",
    "Before downloading we will just query all the sentinel tiles to create a database where we can track which tiles are available to download.\n",
    "\n",
    "Now you can add your details to download the corresponding images. You can choose which images to download depending on the state name or 'all' for downloading all states inside the .shp file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the API\n",
    "\n",
    "username ='andrescamilor'\n",
    "password='ecovision'\n",
    "api = SentinelAPI(username, password, 'https://scihub.copernicus.eu/dhus')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_all = True\n",
    "is_load =False\n",
    "\n",
    "if NAME == 'all':\n",
    "    is_all = True\n",
    "    \n",
    "if is_load:\n",
    "    name_ = '_'.join(NAME).replace(' ','_') if not is_all else 'all'\n",
    "    file_ = glob.glob(f'{save_dir1}/{country}_{name_}_*.pkl')[-1]\n",
    "    \n",
    "    df_download = pd.read_pickle(file_)\n",
    "else:\n",
    "    \n",
    "    products_df = []\n",
    "    # search by polygon, time, and SciHub query keywords\n",
    "    NAME1 = set(df_tiles.index) if is_all else NAME\n",
    "    for name_ in NAME1:\n",
    "        print(name_)\n",
    "        products = api.query(area=features_extent[name_],\n",
    "                             # CHANGE desired time-frame here\n",
    "                             date=('20190101', date(2019, 12,31)),\n",
    "                             processinglevel = 'Level-2A',\n",
    "                             platformname='Sentinel-2')\n",
    "\n",
    "        # convert to Pandas DataFrame\n",
    "        products_df.append(api.to_dataframe(products))\n",
    "\n",
    "\n",
    "\n",
    "    df_out = None\n",
    "    for d_ in products_df:\n",
    "        if df_out is None:\n",
    "            df_out = d_\n",
    "        else:\n",
    "            df_out = df_out.append(d_)\n",
    "\n",
    "    # df_out.shape\n",
    "\n",
    "    df1 = df_out.copy()\n",
    "    df1.drop_duplicates(subset=['title'], inplace =True)\n",
    "#     df1[df1.tileid.isna()].shape, df1[df1.relativeorbitnumber.isna()].shape\n",
    "\n",
    "    df1['sizeMB'] = df1['size'].map(lambda x: float(x.replace(' MB','')) if 'MB' in x else 1000*float(x.replace(' GB','')))\n",
    "    df1['tileid'] = df1['title'].map(lambda x: x.split('_')[5][1:])\n",
    "\n",
    "\n",
    "#     Remove Tiles not in polygon\n",
    "\n",
    "    index = None\n",
    "    for name_ in NAME1:\n",
    "        index_ = [x in roi_tiles_per_feature[name_] for x in df1.tileid]\n",
    "        if index is None:\n",
    "            index = index_\n",
    "        else:\n",
    "            index = np.logical_or(index_,index)\n",
    "\n",
    "\n",
    "    # index\n",
    "    df1 = df1[index]\n",
    "\n",
    "    df_download = df1.sort_values(['cloudcoverpercentage', 'ingestiondate'], ascending=[True, True]).groupby(['tileid','relativeorbitnumber']).head(10)\n",
    "    \n",
    "    d_ = df1[df1.sizeMB > 100].sort_values(['cloudcoverpercentage', 'ingestiondate'], ascending=[True, True]).groupby(['tileid','relativeorbitnumber']).head(10)\n",
    "    df_download = df_download.append(d_)\n",
    "    df_download.drop_duplicates(subset=['title'], inplace =True)\n",
    "    \n",
    "    df_download = df_download.sort_values(['tileid','relativeorbitnumber','cloudcoverpercentage'])\n",
    "    \n",
    "    print(len(np.unique(df_download.tileid)),len(np.unique(df_download.title)))\n",
    "\n",
    "    for counter, (id_, d) in enumerate(df_download.groupby(['tileid','relativeorbitnumber'])):\n",
    "        print(counter, id_,f' N {d.shape[0]} mean cc {d.cloudcoverpercentage.mean():.2f}')\n",
    "\n",
    "    df_download.sizeMB.sum()/(10*60*60)\n",
    "    name_ = '_'.join(NAME).replace(' ','_') if not is_all else 'all'\n",
    "#     name_ = '_'.join(NAME).replace(' ','_')\n",
    "    file_=f'{save_dir1}/{country}_{name_}_{df_download.shape[0]}.pkl'\n",
    "    \n",
    "    df_download.to_pickle(file_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.get_product_odata(df_download.index[10])['Online']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = api.download(df_download.index[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_download[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_download.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1_ = file_.replace('.pkl','.txt')\n",
    "\n",
    "with open(file1_, 'w') as f:\n",
    "    for item in df_download.title:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "print(file1_,'saved')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_download.title[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total size',np.sum(df_download.sizeMB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script could be used to download directly all the scrips but if we have too many tiles, this will usually take several days to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download sorted and reduced products\n",
    "# api.download_all(df.index,directory_path=product_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_download.groupby('processinglevel').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download status and location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(root_dir,'ref_dataframes')\n",
    "\n",
    "dirs_ = [\n",
    "#     root_dir+'/1C/dataframes_download/palmcountries_2017/Malaysia_all_1150.pkl',\n",
    "#          root_dir+'/1C/dataframes_download/palmcountries_2017/Indonesia_all_8410.pkl',\n",
    "#          root_dir+'/1C/dataframes_download/phillipines_2017/Phillipines_all_1840.pkl',\n",
    "#          root_dir+'/1C/dataframes_download/asia_2019/Phillipines_all_1835.pkl',\n",
    "#          root_dir+'/1C/dataframes_download/asia_2019/Malaysia_all_1152.pkl',\n",
    "    root_dir+'/1C/dataframes_download/asia_2019/Indonesia_all_8430.pkl',\n",
    "        ]\n",
    "df_ = [pd.read_pickle(dir_) for dir_ in dirs_]\n",
    "df_ = pd.concat(df_)\n",
    "\n",
    "df_['1C_path'] = None\n",
    "df_['2A_path'] = None\n",
    "df_['correct2A'] = None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path =f'{root_dir}/1C/*/PRODUCT/'\n",
    "# base_path ='/home/pf/pfstaff/projects/andresro/barry_palm/data/1C/palm_2017/PRODUCT/'\n",
    "print(base_path)\n",
    "filelist = glob.glob(base_path+'*.zip')\n",
    "\n",
    "titlelist = [os.path.split(x)[-1].replace('.zip','') for x in filelist]\n",
    "def path_if_exists(x):\n",
    "    if x['1C_path'] is None:\n",
    "        if x['title'] in titlelist:\n",
    "            return filelist[titlelist.index(x['title'])]\n",
    "    return x['1C_path']\n",
    "\n",
    "df_['1C_path'] = df_.apply(path_if_exists,axis=1)\n",
    "\n",
    "\n",
    "base_path =f'{root_dir}/2A/*/'\n",
    "# base_path ='/home/pf/pfstaff/projects/andresro/barry_palm/data/1C/palm_2017/PRODUCT/'\n",
    "print(base_path)\n",
    "filelist = glob.glob(base_path+'*.SAFE')\n",
    "\n",
    "titlelist = [os.path.split(x)[-1].replace('.SAFE','') for x in filelist]\n",
    "def path_if_exists(x):\n",
    "    if x['2A_path'] is None:\n",
    "        title_ = x['title'].replace('_MSIL1C_','_MSIL2A_')\n",
    "        if title_ in titlelist:\n",
    "            return filelist[titlelist.index(title_)]\n",
    "    return x['2A_path']\n",
    "\n",
    "df_['2A_path'] = df_.apply(path_if_exists,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check2A(x):\n",
    "    if x['correct2A'] is None and x['2A_path'] is not None:\n",
    "        file_ = x['2A_path']+'/jp2count.txt'\n",
    "        if not os.path.isfile(file_):\n",
    "            jp2count = len(glob.glob(x['2A_path']+'/**/*.jp2', recursive=True))\n",
    "            f = open(file_, \"w\")\n",
    "            f.write(str(jp2count))\n",
    "            f.close()\n",
    "        else:\n",
    "            f = open(file_, \"r\")\n",
    "            jp2count = int(f.read())\n",
    "            f.close()\n",
    "        return jp2count\n",
    "    return x['correct2A']\n",
    "    \n",
    "df_['correct_2A'] = df_.apply(check2A,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_.shape)\n",
    "'1Czip: ',np.sum(~df_['1C_path'].isna()), '2A:', np.sum(~df_['2A_path'].isna()), 'correct2A:',np.sum(df_['correct_2A'].dropna() >= 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE DF\n",
    "df_.to_pickle(root_dir+'/filestatus.pkl')\n",
    "print(root_dir+'/filestatus.pkl','saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check 1C downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles = df_download.title.map(lambda x: '_'.join(x.split('_')[4:6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_path =root_dir+'/2A/{}/PRODUCT/'.format(loc) \n",
    "# base_path ='/home/pf/pfstaff/projects/andresro/barry_palm/data/1C/palm_2017/PRODUCT/'\n",
    "\n",
    "filelist = glob.glob(base_path+'*.zip')\n",
    "\n",
    "existing_ds = [os.path.split(x)[-1].replace('.zip','') for x in filelist]\n",
    "# pending_ds = [x for x in df_download.title if x not in existing_ds]\n",
    "pending_ds = [x not in existing_ds for x in df_download.title]\n",
    "\n",
    "print('total',df_download.shape[0])\n",
    "print(f'existing {len(existing_ds)} in {base_path}')\n",
    "print('pending',np.sum(pending_ds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_existing = df_download[~np.array(pending_ds)]\n",
    "\n",
    "file_existing_correct = base_path+'/correct_zip.txt'\n",
    "lines = [line.rstrip('\\n') for line in open(file_existing_correct)]\n",
    "\n",
    "is_checked = [x not in lines for x in df_existing.title]\n",
    "df_to_check = df_existing[is_checked]\n",
    "\n",
    "print('1C ds pending checksum:',df_to_check.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id_, row in df_download.head(5).iterrows():\n",
    "    print(row.link)\n",
    "#     print(f'wget --content-disposition --continue --user={username} --password={password} \"https://scihub.copernicus.eu/dhus/odata/v1/Products(\\'{id_}\\')/\\$value\" -P {save_dir}')\n",
    "#     print(f'wget --content-disposition --continue --user={username} --password={password} \"{row.link}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_download.inde\n",
    "base_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check 2A files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=root_dir+'/1C/'+loc+'/PRODUCT/correct_zip.txt'\n",
    "lines1C = [line.rstrip('\\n') for line in open(path)]\n",
    "lines1C = [x for x in lines1C if '2017' in x]\n",
    "print('1C: ',len(lines1C))\n",
    "\n",
    "\n",
    "path=root_dir+'/2A/'+loc+'/correct_2A.txt'\n",
    "lines2A = [line.rstrip('\\n') for line in open(path)]\n",
    "lines2A = [x for x in lines2A if '2017' in x]\n",
    "print('2A: ',len(lines2A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1C = pd.DataFrame({'title1C': lines1C})\n",
    "\n",
    "ds1C['tile'] = ds1C.title1C.map(lambda x: x.split('_')[5])\n",
    "ds1C['orbit'] = ds1C.title1C.map(lambda x: x.split('_')[4])\n",
    "\n",
    "ds1C.head()\n",
    "\n",
    "counts1C = ds1C.groupby(['tile','orbit']).count().rename({'title1C':'count1C'},axis=1)\n",
    "\n",
    "ds1C = ds1C.set_index(['tile','orbit']).join(counts1C)\n",
    "ds1C['title2A'] = ds1C.title1C.map(lambda x: x.replace('MSIL1C','MSIL2A'))\n",
    "ds1C.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2A = pd.DataFrame({'title2A': lines2A})\n",
    "\n",
    "ds2A['tile'] = ds2A.title2A.map(lambda x: x.split('_')[5])\n",
    "ds2A['orbit'] = ds2A.title2A.map(lambda x: x.split('_')[4])\n",
    "\n",
    "ds2A.head()\n",
    "\n",
    "counts2A = ds2A.groupby(['tile','orbit']).count().rename({'title2A':'count2A'},axis=1)\n",
    "ds2A = ds2A.set_index(['tile','orbit']).join(counts2A)\n",
    "ds2A['correct2A'] = True\n",
    "\n",
    "ds2A.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsAll = ds1C.set_index('title2A').join(ds2A.set_index('title2A'))\n",
    "\n",
    "#dsAll[dsAll.correct2A != True].title1C\n",
    "#dsAll[dsAll.title1C]\n",
    "dsAll.reset_index().columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsAll[dsAll.correct2A != True].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds2A.groupby(['tile']).count()\n",
    "\n",
    "counts2A.sort_index().sort_values(by='count2A',ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pending by tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_pickle('/home/pf/pfstaff/projects/andresro/barry_palm/data/dataframes_download/palmcountries_2017/Indonesia_all_8410.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_pending = ['47MRS','47NLC','49LHL','49MGS']\n",
    "\n",
    "df_out = None\n",
    "for tile_ in tiles_pending:\n",
    "    products = api.query(tileid=tile_,\n",
    "    #                      area=features_extent['Bengkulu'],\n",
    "                         # CHANGE desired time-frame here\n",
    "                         date=('20170101', date(2017, 12,31)),\n",
    "                                 processinglevel = 'Level-1C',\n",
    "                                 platformname='Sentinel-2')\n",
    "\n",
    "\n",
    "    df1 = api.to_dataframe(products)\n",
    "    df_out = df_out.append(df1) if df_out is not None else df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual_dsets = [\"S2A_MSIL1C_20171113T034011_N0206_R061_T47NLC_20171113T085706\",\n",
    "#  \"S2B_MSIL1C_20171208T034119_N0206_R061_T47NLC_20171208T090517\",\n",
    "#  \"S2B_MSIL1C_20171218T034139_N0206_R061_T47NLC_20171218T085501\",\n",
    "#  \"S2A_MSIL1C_20171223T034141_N0206_R061_T47NLC_20171223T085847\",\n",
    "#  \"S2A_MSIL1C_20170716T033541_N0205_R061_T47NLC_20170716T035428\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "df1 = df_out.copy()\n",
    "df1.drop_duplicates(subset=['title'], inplace =True)\n",
    "\n",
    "df1['sizeMB'] = df1['size'].map(lambda x: float(x.replace(' MB','')) if 'MB' in x else 1000*float(x.replace(' GB','')))\n",
    "df1['tileid'] = df1['title'].map(lambda x: x.split('_')[5][1:])\n",
    "\n",
    "\n",
    "df_download = df1.sort_values(['cloudcoverpercentage', 'ingestiondate'], ascending=[True, True]).groupby(['tileid','relativeorbitnumber']).head(10)\n",
    "\n",
    "# d_ = df1[[x in manual_dsets for x in df1.title]]\n",
    "# df_download = df_download.append(d_)\n",
    "\n",
    "# added to avoid getting only the top dsets of small cuts as it happened in 47MRS and 47NLC\n",
    "d_ = df1[df1.sizeMB > 100].sort_values(['cloudcoverpercentage', 'ingestiondate'], ascending=[True, True]).groupby(['tileid','relativeorbitnumber']).head(10)\n",
    "df_download = df_download.append(d_)\n",
    "\n",
    "df_download.drop_duplicates(subset=['title'], inplace =True)\n",
    "\n",
    "df_download = df_download.sort_values(['tileid','relativeorbitnumber','cloudcoverpercentage'])\n",
    "\n",
    "# remote existing 2A\n",
    "\n",
    "convert2a = lambda x: x.replace('_MSIL2A_','_MSIL1C_')+'.SAFE'\n",
    "\n",
    "path='/scratch/andresro/leon_work/barry_palm/data/2A/palmcountries_2017/correct_2A.txt'\n",
    "lines = [convert2a(line.rstrip('\\n')) for line in open(path)]\n",
    "existing2a = np.array([x in lines for x in df_download.filename])\n",
    "\n",
    "\n",
    "df_download = df_download[~existing2a]\n",
    "\n",
    "\n",
    "print(len(np.unique(df_download.tileid)),len(np.unique(df_download.title)))\n",
    "\n",
    "for counter, (id_, d) in enumerate(df_download.groupby(['tileid','relativeorbitnumber'])):\n",
    "    print(counter, id_,f' N {d.shape[0]} mean cc {d.cloudcoverpercentage.mean():.2f}')\n",
    "\n",
    "df_download.sizeMB.sum()/(10*60*60)\n",
    "file_ = f'{root_dir}/dataframes_download/palmcountries_2017/palmpending_{df_download.shape[0]}.pkl'\n",
    "#name_ = '_'.join(NAME).replace(' ','_') if not is_all else 'all'\n",
    "#     name_ = '_'.join(NAME).replace(' ','_')\n",
    "#file_=f'{save_dir1}/{country}_{name_}_{df_download.shape[0]}.pkl'\n",
    "print(file_)\n",
    "df_download.to_pickle(file_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df_download[df_download.relativeorbitnumber==61].title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in lines if '_R061_T47NLC_' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert2a = lambda x: x.replace('_MSIL2A_','_MSIL1C_')+'.SAFE'\n",
    "\n",
    "path='/scratch/andresro/leon_work/barry_palm/data/2A/palmcountries_2017/correct_2A.txt'\n",
    "lines = [convert2a(line.rstrip('\\n')) for line in open(path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df1.sort_values(\n",
    "    ['cloudcoverpercentage', 'ingestiondate'], ascending=[True, True]\n",
    "               ).groupby(['tileid','relativeorbitnumber']).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing2a = np.array([x in lines for x in df_.filename])\n",
    "print(existing2a.sum(), df_.shape)\n",
    "\n",
    "\n",
    "sorted(df_[~existing2a].filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df_download[df_download['tileid'] == '47NLC'].title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in lines if 'R061_T47NLC' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.logical_and(df_download.relativeorbitnumber == 61, df_download.tileid == '47NLC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_download[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_download.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get tiles ready to predict on asia_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/scratch/andresro/leon_work/barry_palm/data/2A/asia_2019/correct_2A.txt'\n",
    "lines = [convert2a(line.rstrip('\\n')) for line in open(path)]\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_folders = glob.glob('/scratch/andresro/leon_work/barry_palm/data/2A/asia_2019/PRODUCT/*.SAFE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines1 = [os.path.basename(x) for x in safe_folders]\n",
    "print(len(lines1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines = lines1\n",
    "lines.extend(lines1)\n",
    "\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_ = set([x.split('_')[5] for x in lines])\n",
    "print(len(tiles_))\n",
    "' '.join(sorted(tiles_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bjobs 11521887"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gdal233]",
   "language": "python",
   "name": "conda-env-gdal233-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
